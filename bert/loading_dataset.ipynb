{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'species_name': 'botrytis_porri_gca_004786265',\n",
       " '__index_level_0__': 1077855,\n",
       " 'input_ids': [2,\n",
       "  4203,\n",
       "  1030,\n",
       "  12,\n",
       "  33,\n",
       "  120,\n",
       "  466,\n",
       "  1850,\n",
       "  3292,\n",
       "  865,\n",
       "  3445,\n",
       "  1478,\n",
       "  1804,\n",
       "  3108,\n",
       "  129,\n",
       "  502,\n",
       "  1994,\n",
       "  3868,\n",
       "  3169,\n",
       "  373,\n",
       "  1477,\n",
       "  1797,\n",
       "  3078,\n",
       "  9,\n",
       "  22,\n",
       "  74,\n",
       "  281,\n",
       "  1112,\n",
       "  337,\n",
       "  1334,\n",
       "  1226,\n",
       "  794,\n",
       "  3161,\n",
       "  343,\n",
       "  1357,\n",
       "  1317,\n",
       "  1160,\n",
       "  531,\n",
       "  2112,\n",
       "  242,\n",
       "  953,\n",
       "  3798,\n",
       "  2889,\n",
       "  3349,\n",
       "  1096,\n",
       "  276,\n",
       "  1089,\n",
       "  246,\n",
       "  971,\n",
       "  3870,\n",
       "  3179,\n",
       "  416,\n",
       "  1650,\n",
       "  2492,\n",
       "  1761,\n",
       "  2934,\n",
       "  3531,\n",
       "  1823,\n",
       "  3182,\n",
       "  426,\n",
       "  1691,\n",
       "  2654,\n",
       "  2411,\n",
       "  1438,\n",
       "  1642,\n",
       "  2458,\n",
       "  1626,\n",
       "  2394,\n",
       "  1370,\n",
       "  1371,\n",
       "  1375,\n",
       "  1390,\n",
       "  1450,\n",
       "  1691,\n",
       "  2655,\n",
       "  2414,\n",
       "  1451,\n",
       "  1694,\n",
       "  2666,\n",
       "  2459,\n",
       "  1630,\n",
       "  2410,\n",
       "  1434,\n",
       "  1627,\n",
       "  2398,\n",
       "  1386,\n",
       "  1434,\n",
       "  1626,\n",
       "  2395,\n",
       "  1374,\n",
       "  1388,\n",
       "  1444,\n",
       "  1668,\n",
       "  2561,\n",
       "  2037,\n",
       "  4039,\n",
       "  3854,\n",
       "  3113,\n",
       "  150,\n",
       "  586,\n",
       "  2330,\n",
       "  1116,\n",
       "  353,\n",
       "  1400,\n",
       "  1491,\n",
       "  1856,\n",
       "  3314,\n",
       "  956,\n",
       "  3810,\n",
       "  2939,\n",
       "  3549,\n",
       "  1894,\n",
       "  3465,\n",
       "  1558,\n",
       "  2121,\n",
       "  280,\n",
       "  1108,\n",
       "  324,\n",
       "  1282,\n",
       "  1017,\n",
       "  4056,\n",
       "  3922,\n",
       "  3387,\n",
       "  1245,\n",
       "  871,\n",
       "  3470,\n",
       "  1577,\n",
       "  2200,\n",
       "  596,\n",
       "  2372,\n",
       "  1282,\n",
       "  1018,\n",
       "  4060,\n",
       "  3940,\n",
       "  3457,\n",
       "  1528,\n",
       "  2002,\n",
       "  3898,\n",
       "  3289,\n",
       "  853,\n",
       "  3398,\n",
       "  1292,\n",
       "  1060,\n",
       "  132,\n",
       "  516,\n",
       "  2050,\n",
       "  4090,\n",
       "  4058,\n",
       "  3932,\n",
       "  3425,\n",
       "  1398,\n",
       "  1482,\n",
       "  1820,\n",
       "  3169,\n",
       "  376,\n",
       "  1492,\n",
       "  1858,\n",
       "  3324,\n",
       "  994,\n",
       "  3961,\n",
       "  3543,\n",
       "  1870,\n",
       "  3370,\n",
       "  1180,\n",
       "  609,\n",
       "  2422,\n",
       "  1483,\n",
       "  1822,\n",
       "  3177,\n",
       "  406,\n",
       "  1611,\n",
       "  2333,\n",
       "  1128,\n",
       "  402,\n",
       "  1596,\n",
       "  2273,\n",
       "  885,\n",
       "  3526,\n",
       "  1804,\n",
       "  3105,\n",
       "  118,\n",
       "  457,\n",
       "  1813,\n",
       "  3143,\n",
       "  269,\n",
       "  1062,\n",
       "  140,\n",
       "  545,\n",
       "  2166,\n",
       "  460,\n",
       "  1828,\n",
       "  3201,\n",
       "  502,\n",
       "  1994,\n",
       "  3866,\n",
       "  3162,\n",
       "  345,\n",
       "  1366,\n",
       "  1354,\n",
       "  1306,\n",
       "  1113,\n",
       "  344,\n",
       "  1362,\n",
       "  1338,\n",
       "  1241,\n",
       "  854,\n",
       "  3403,\n",
       "  1312,\n",
       "  1137,\n",
       "  439,\n",
       "  1742,\n",
       "  2858,\n",
       "  3228,\n",
       "  610,\n",
       "  2425,\n",
       "  1494,\n",
       "  1868,\n",
       "  3361,\n",
       "  1141,\n",
       "  454,\n",
       "  1801,\n",
       "  3095,\n",
       "  77,\n",
       "  295,\n",
       "  1168,\n",
       "  562,\n",
       "  2234,\n",
       "  730,\n",
       "  2906,\n",
       "  3418,\n",
       "  1370,\n",
       "  1371,\n",
       "  1374,\n",
       "  1388,\n",
       "  1441,\n",
       "  1654,\n",
       "  2505,\n",
       "  1814,\n",
       "  3147,\n",
       "  288,\n",
       "  1140,\n",
       "  449,\n",
       "  1781,\n",
       "  3014,\n",
       "  3850,\n",
       "  3100,\n",
       "  100,\n",
       "  387,\n",
       "  1535,\n",
       "  2029,\n",
       "  4008,\n",
       "  3730,\n",
       "  2618,\n",
       "  2266,\n",
       "  857,\n",
       "  3413,\n",
       "  1350,\n",
       "  1290,\n",
       "  1049,\n",
       "  88,\n",
       "  338,\n",
       "  1337,\n",
       "  1238,\n",
       "  842,\n",
       "  3354,\n",
       "  1114,\n",
       "  348,\n",
       "  1378,\n",
       "  1404,\n",
       "  1506,\n",
       "  1916,\n",
       "  3554,\n",
       "  1916,\n",
       "  3554,\n",
       "  1914,\n",
       "  3548,\n",
       "  1889,\n",
       "  3446,\n",
       "  1484,\n",
       "  1826,\n",
       "  3195,\n",
       "  480,\n",
       "  1906,\n",
       "  3514,\n",
       "  1754,\n",
       "  2905,\n",
       "  3414,\n",
       "  1354,\n",
       "  1308,\n",
       "  1122,\n",
       "  378,\n",
       "  1500,\n",
       "  1892,\n",
       "  3458,\n",
       "  1532,\n",
       "  2018,\n",
       "  3964,\n",
       "  3],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = dataset[\"train\"]\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species_name botrytis_porri_gca_004786265\n",
      "__index_level_0__  int  1077855\n",
      "input_ids 298\n",
      "token_type_ids 298\n",
      "attention_mask 298\n"
     ]
    }
   ],
   "source": [
    "#print(train_data[0].values())\n",
    "for k, v in train_data[0].items():\n",
    "    if isinstance(v, str):\n",
    "        print(k, v)\n",
    "    elif isinstance(v, np.ndarray):\n",
    "        print(k, v.shape)\n",
    "    elif isinstance(v, int):\n",
    "        print(k, \" int \",  v)\n",
    "    else:\n",
    "        print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data = train_data.remove_columns([\"species_name\", \"__index_level_0__\"])\n",
    "collate_fn = DataCollatorForLanguageModeling(\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"zhihan1996/DNA_bert_6\", trust_remote_code=True))\n",
    "dataloader = DataLoader(train_data, batch_size=16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n",
      "torch.Size([16, 298])\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataloader)\n",
    "batch = next(iterator)\n",
    "for i in range(100):\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    batch = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['species_name', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/.conda/envs/m2_bert/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for eli5_category contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/eli5_category\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "eli5 = load_dataset(\"eli5_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '5lchat',\n",
       " 'title': \"Why there was a 'leap second' added to the end of 2016?\",\n",
       " 'selftext': '',\n",
       " 'category': 'Other',\n",
       " 'subreddit': 'explainlikeimfive',\n",
       " 'answers': {'a_id': ['dbuoyxl', 'dbur7gi', 'dbuotht'],\n",
       "  'text': ['the rotation of the earth is not a constant. in fact the rotation of the earth is slowing down, which means that a full day is getting slightly longer. without leap seconds our clocks would slowly drift ever so slightly out of sync with the actual day. we could deal with this by redefining how how long 1 second is, making it slightly longer so that one day is still exactly 24*60*60 seconds. but in practice that is really inconvenient for a lot of our technology which relies on very precise timing. its easier to just move us ahead one second every couple of years or so.',\n",
       "   \"The Earth's rotation is not regular. It varies a bit, so sometimes we add a second. We do this to ensure that noon is always going to be sometime around mid-day. If we did not add leap seconds, over a very long period of time where the Earth's rotation slowly changed, noon could end up being at dusk. We want to keep 7am in the morning, noon at mid-day, 7pm around evening, etc. Though we have never had one, it's also possible to have a negative leap second. That is, taking away a second from the year. This has never happened, but if the Earth's rotation were to speed up, it could happen. The biggest thing to know about leap seconds is that they can cause computer problems. You might remember the Y2K bug. A leap second can cause similar problems, and they actually have caused problems in the past. The reason for this is that generally we expect a day to have 24 hours, and for time to always move forward. With a leap second this is not true. When writing software, programers try to think of all the possible exceptions that could happen withing their code. For example, the program might expect a word, but instead get a number. A good programmer will check for these exceptions and deal with them. However, a programer can easily forget about leap seconds and not have a fail-safe in their code for when a day have more than 24 hours. When such an exception happens, the program can produce errors or crash. It is an interesting topic, you can read more about it here: URL_0\",\n",
       "   \"Because the Earth's rotation is slowing. If you multiply 24 hours by 60 minutes by 60 seconds, you find that there are 86400 seconds per day. The problem is that our definition of the second is based on [an average that is a century old.]( URL_0 ) In modern times, the average day is about 2 thousandths of a second longer—again, because of Earth's slowing rotation. Those thousandths of a second add up, so every few years we have to slip in an extra second to account for them. Without leap seconds, we'd eventually end up with noon at 7 o'clock, though admittedly, this would take a very long time.\"],\n",
       "  'score': [44, 5, 4],\n",
       "  'text_urls': [[],\n",
       "   ['http://adminhacks.com/leap-second-bugs.html'],\n",
       "   ['https://en.wikipedia.org/wiki/Newcomb%27s_Tables_of_the_Sun']]},\n",
       " 'title_urls': ['url'],\n",
       " 'selftext_urls': ['url']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):   0%|          | 0/91772 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1510 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=4):   1%|          | 1000/91772 [00:00<01:15, 1206.20 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (776 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=4): 100%|██████████| 91772/91772 [00:21<00:00, 4229.03 examples/s]\n",
      "Map (num_proc=4):   0%|          | 0/5446 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=4):  18%|█▊        | 1000/5446 [00:01<00:05, 859.05 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2657 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=4):  43%|████▎     | 2361/5446 [00:01<00:01, 1815.09 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (746 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1403 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=4): 100%|██████████| 5446/5446 [00:02<00:00, 2410.42 examples/s]\n",
      "Map (num_proc=4):   0%|          | 0/2375 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (871 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (671 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2559 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=4):  25%|██▍       | 593/2375 [00:00<00:01, 943.55 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=4): 100%|██████████| 2375/2375 [00:00<00:00, 2964.87 examples/s]\n",
      "Map (num_proc=4):   0%|          | 0/5411 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (568 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1430 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=4):  18%|█▊        | 1000/5411 [00:00<00:03, 1195.94 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (986 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (714 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=4): 100%|██████████| 5411/5411 [00:01<00:00, 3394.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "eli5 = eli5.flatten()\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])\n",
    "tokenized_eli6 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched = True,\n",
    "    num_proc = 4,\n",
    "    remove_columns = eli5[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate sequences\n",
    "block_size = 128\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "\n",
    "    # Concatenate all texts.\n",
    "\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "\n",
    "    # customize this part to your needs.\n",
    "\n",
    "    if total_length >= block_size:\n",
    "\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "\n",
    "    # Split by chunks of block_size.\n",
    "\n",
    "    result = {\n",
    "\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "\n",
    "        for k, t in concatenated_examples.items()\n",
    "\n",
    "    }\n",
    "\n",
    "    return result\n",
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "\n",
    "\n",
    "collate_fn = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128])\n",
      "input_ids\n",
      "torch.Size([16, 128])\n",
      "attention_mask\n",
      "torch.Size([16, 128])\n",
      "labels\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(lm_dataset[\"train\"], batch_size=16, collate_fn=collate_fn)\n",
    "iterable = iter(loader)\n",
    "batch = next(iterable)\n",
    "for k, v in batch.items():\n",
    "    print(v.shape)\n",
    "    print(k)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "input_ids\n",
      "128\n",
      "attention_mask\n"
     ]
    }
   ],
   "source": [
    "for k, v in lm_dataset[\"train\"][0].items():\n",
    "    print(len(v))\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from composer import Trainer\n",
    "from composer.models import ComposerClassifier\n",
    "from composer.algorithms import LabelSmoothing, CutMix, ChannelsLast\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"Toy convolutional neural network architecture in pytorch for MNIST.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, (3, 3), padding=0)\n",
    "        self.conv2 = nn.Conv2d(16, 32, (3, 3), padding=0)\n",
    "        self.bn = nn.BatchNorm2d(32)\n",
    "        self.fc1 = nn.Linear(32 * 16, 32)\n",
    "        self.fc2 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.adaptive_avg_pool2d(out, (4, 4))\n",
    "        out = torch.flatten(out, 1, -1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        return self.fc2(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/.conda/envs/m2_bert/lib/python3.10/site-packages/composer/trainer/trainer.py:968: UserWarning: No optimizer was specified. Defaulting to DecoupledSGDW(lr=0.1)\n",
      "  warnings.warn(('No optimizer was specified. Defaulting to '\n",
      "******************************\n",
      "Config:\n",
      "enabled_algorithms/ChannelsLast: true\n",
      "enabled_algorithms/CutMix: true\n",
      "enabled_algorithms/LabelSmoothing: true\n",
      "node_name: unknown because NODENAME environment variable not set\n",
      "num_cpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 4240981184\n",
      "\n",
      "******************************\n",
      "train          Epoch   0:    0%|| 0/14426 [00:00<?, ?ba/s]         "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([transforms\u001b[38;5;241m.\u001b[39mToTensor()])\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mComposerClassifier(module\u001b[38;5;241m=\u001b[39mModel(), num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m      4\u001b[0m     train_dataloader\u001b[38;5;241m=\u001b[39mloader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     ],\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/m2_bert/lib/python3.10/site-packages/composer/trainer/trainer.py:1766\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_dataloader, train_dataloader_label, train_subset_num_batches, duration, reset_time, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_subset_num_batches, eval_interval, device_train_microbatch_size, precision)\u001b[0m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m ClosureGradScaler() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_closures() \u001b[38;5;28;01melse\u001b[39;00m GradScaler()\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst_batch_complete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1766\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/m2_bert/lib/python3.10/site-packages/composer/trainer/trainer.py:1919\u001b[0m, in \u001b[0;36mTrainer._train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mbatch_to_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[1;32m   1918\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_data_spec\u001b[38;5;241m.\u001b[39mdevice_transforms(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m-> 1919\u001b[0m rank_num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_data_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_samples_in_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m rank_num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_data_spec\u001b[38;5;241m.\u001b[39mget_num_tokens_in_batch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed_enabled:\n",
      "File \u001b[0;32m~/.conda/envs/m2_bert/lib/python3.10/site-packages/composer/core/data_spec.py:252\u001b[0m, in \u001b[0;36mDataSpec._default_get_num_samples_in_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    250\u001b[0m             dim0_sizes\u001b[38;5;241m.\u001b[39mappend(t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 252\u001b[0m     dim0_sizes \u001b[38;5;241m=\u001b[39m [t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(dim0_sizes)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dim0_sizes[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/m2_bert/lib/python3.10/site-packages/composer/core/data_spec.py:252\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m             dim0_sizes\u001b[38;5;241m.\u001b[39mappend(t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 252\u001b[0m     dim0_sizes \u001b[38;5;241m=\u001b[39m [\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(dim0_sizes)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dim0_sizes[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainer = Trainer(\n",
    "    model=ComposerClassifier(module=Model(), num_classes=10),\n",
    "    train_dataloader=loader,\n",
    "    max_duration=\"2ep\",\n",
    "    algorithms=[\n",
    "        LabelSmoothing(smoothing=0.1),\n",
    "        CutMix(alpha=1.0),\n",
    "        ChannelsLast(),\n",
    "    ],\n",
    ")\n",
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
