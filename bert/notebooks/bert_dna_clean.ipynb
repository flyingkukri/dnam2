{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "You are using a model of type m2_bert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Bidirectional: True\n",
      "-- Using Long Conv Residual: True\n",
      "-- Hyena w: 10\n",
      "-- Hyena w mod: 1\n",
      "-- Hyena filter order: 128\n",
      "-- Hyena filter dropout: 0.2\n",
      "-- Hyena filter wd: 0.1\n",
      "-- Hyena filter emb dim: 5\n",
      "-- Hyena filter lr: 0.001\n",
      "-- Hyena filter lr pos emb: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at togethercomputer/m2-bert-80M-2k were not used when initializing BertForMaskedLM: ['model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter.4.weight', 'model.bert.encoder.layer.4.attention.filter_fn.pos_emb.t', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter.2.weight', 'model.bert.encoder.layer.4.mlp.layernorm.weight', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter.5.freq', 'model.bert.encoder.layer.9.attention.filter_fn.pos_emb.z', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter.5.freq', 'model.bert.embeddings.token_type_embeddings.weight', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter.1.freq', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter.5.freq', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter.4.weight', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.8.attention.filter_fn2.pos_emb.z', 'model.bert.embeddings.LayerNorm.bias', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter.4.bias', 'model.bert.encoder.layer.2.attention.filter_fn.pos_emb.t', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter.3.freq', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter.4.bias', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter.2.weight', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.1.attention.filter_fn.modulation.deltas', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.5.attention.filter_fn2.pos_emb.t', 'model.bert.encoder.layer.2.attention.filter_fn2.pos_emb.z', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.6.mlp.layernorm.weight', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter.6.weight', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter.2.bias', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter.6.weight', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter.5.freq', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter.1.freq', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter.0.weight', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter_rev.4.weight', 'model.bert.embeddings.position_embeddings.weight', 'model.bert.encoder.layer.4.attention.out_linear.weight', 'model.bert.encoder.layer.11.attention.out_linear.weight', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter.4.bias', 'model.bert.encoder.layer.1.mlp.layernorm.weight', 'model.bert.encoder.layer.7.attention.filter_fn.bias', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter.6.weight', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter.0.weight', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter.4.bias', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter.2.bias', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter.4.weight', 'model.bert.encoder.layer.7.mlp.wo.weight', 'model.bert.encoder.layer.10.attention.filter_fn.modulation.deltas', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter.3.freq', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter.2.bias', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter.5.freq', 'model.bert.encoder.layer.9.attention.short_filter.weight', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter.6.weight', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter.6.weight', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter.1.freq', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter.4.bias', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter.0.weight', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter.4.bias', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.11.attention.filter_fn2.modulation.deltas', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter.2.bias', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter.0.bias', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter.2.bias', 'model.bert.encoder.layer.10.mlp.wo.weight', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.5.mlp.wo.weight', 'model.bert.encoder.layer.0.mlp.wo.bias', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter.0.bias', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.0.attention.in_linear.weight', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter.4.weight', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter.3.freq', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.10.attention.filter_fn2.pos_emb.t', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter.6.weight', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter.6.weight', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.1.attention.short_filter.bias', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter.2.weight', 'model.bert.encoder.layer.0.attention.filter_fn.pos_emb.t', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.10.attention.filter_fn.bias', 'model.bert.encoder.layer.9.attention.filter_fn2.pos_emb.t', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter.4.bias', 'model.bert.encoder.layer.0.mlp.gated_layers.weight', 'model.bert.encoder.layer.6.mlp.gated_layers.weight', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.3.attention.filter_fn2.pos_emb.t', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.0.attention.filter_fn2.modulation.deltas', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter.0.bias', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter.0.weight', 'model.bert.encoder.layer.2.attention.filter_fn2.pos_emb.t', 'model.bert.encoder.layer.4.attention.in_linear.weight', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter.4.bias', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter.3.freq', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter.3.freq', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.8.attention.filter_fn.modulation.deltas', 'model.bert.encoder.layer.6.attention.filter_fn.pos_emb.t', 'model.bert.encoder.layer.6.attention.in_linear.bias', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter.5.freq', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter.0.bias', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter.5.freq', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.9.attention.filter_fn2.pos_emb.z', 'model.bert.encoder.layer.11.attention.filter_fn2.bias', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.2.mlp.gated_layers.weight', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter.4.weight', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter.1.freq', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.5.attention.filter_fn.modulation.deltas', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter.4.bias', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter.5.freq', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.10.attention.short_filter.weight', 'model.bert.encoder.layer.10.mlp.wo.bias', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter.2.bias', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter.2.bias', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter.3.freq', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter.2.weight', 'model.bert.encoder.layer.7.mlp.layernorm.weight', 'model.cls.predictions.transform.dense.bias', 'model.bert.encoder.layer.3.attention.in_linear.bias', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter.0.weight', 'model.bert.encoder.layer.2.attention.short_filter.weight', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.9.mlp.wo.weight', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter.0.bias', 'model.bert.encoder.layer.6.attention.filter_fn2.modulation.deltas', 'model.cls.predictions.transform.LayerNorm.weight', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.7.attention.filter_fn2.modulation.deltas', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.7.attention.filter_fn.pos_emb.t', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter.5.freq', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter.4.weight', 'model.bert.encoder.layer.3.attention.short_filter.weight', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter.4.weight', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter.6.weight', 'model.bert.encoder.layer.2.attention.filter_fn.pos_emb.z', 'model.bert.encoder.layer.10.attention.in_linear.bias', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter.4.bias', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter.4.weight', 'model.bert.encoder.layer.0.attention.filter_fn2.bias', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter.5.freq', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter.2.weight', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.6.attention.filter_fn.bias', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter_rev.1.freq', 'model.bert.embeddings.LayerNorm.weight', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter.2.bias', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter.2.weight', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter.5.freq', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.8.attention.filter_fn.bias', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter.3.freq', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter.1.freq', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter.5.freq', 'model.bert.encoder.layer.1.attention.in_linear.weight', 'model.bert.encoder.layer.6.attention.out_linear.weight', 'model.bert.encoder.layer.1.attention.out_linear.weight', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.1.attention.filter_fn.pos_emb.z', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter.6.weight', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter.5.freq', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.10.attention.filter_fn.pos_emb.z', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter.4.weight', 'model.bert.encoder.layer.5.attention.filter_fn2.bias', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter.6.weight', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter.5.freq', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.8.mlp.layernorm.bias', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.3.attention.filter_fn.pos_emb.t', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter.3.freq', 'model.bert.encoder.layer.1.attention.filter_fn2.modulation.deltas', 'model.bert.encoder.layer.7.attention.filter_fn2.bias', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter.6.weight', 'model.bert.encoder.layer.5.attention.filter_fn2.modulation.deltas', 'model.bert.encoder.layer.8.attention.in_linear.weight', 'model.bert.encoder.layer.1.attention.filter_fn2.bias', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter.1.freq', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter_rev.6.weight', 'model.cls.predictions.transform.dense.weight', 'model.bert.encoder.layer.2.mlp.layernorm.weight', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter.0.weight', 'model.bert.encoder.layer.0.attention.filter_fn2.pos_emb.t', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter.2.bias', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter.2.bias', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter.1.freq', 'model.bert.encoder.layer.6.mlp.layernorm.bias', 'model.bert.encoder.layer.10.attention.in_linear.weight', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter.1.freq', 'model.bert.encoder.layer.10.attention.short_filter.bias', 'model.bert.encoder.layer.2.attention.filter_fn2.bias', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter.2.bias', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter.6.weight', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter.3.freq', 'model.bert.encoder.layer.11.attention.filter_fn2.pos_emb.z', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.3.attention.short_filter.bias', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter.4.bias', 'model.bert.encoder.layer.2.attention.in_linear.weight', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.8.attention.filter_fn2.pos_emb.t', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.11.attention.filter_fn.pos_emb.t', 'model.bert.encoder.layer.11.mlp.wo.weight', 'model.bert.encoder.layer.1.mlp.wo.weight', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.7.attention.filter_fn2.pos_emb.z', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.3.attention.out_linear.weight', 'model.bert.encoder.layer.3.attention.in_linear.weight', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.0.mlp.layernorm.bias', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.8.attention.out_linear.weight', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter.5.freq', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter.6.weight', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.4.mlp.gated_layers.weight', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter.0.bias', 'model.bert.encoder.layer.3.attention.filter_fn2.bias', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter.4.bias', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter.6.weight', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter.3.freq', 'model.bert.encoder.layer.3.mlp.layernorm.weight', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter.2.weight', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter.6.weight', 'model.bert.encoder.layer.11.attention.in_linear.bias', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter.6.weight', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter.3.freq', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter.6.weight', 'model.bert.encoder.layer.5.attention.out_linear.bias', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.0.mlp.layernorm.weight', 'model.bert.encoder.layer.3.mlp.wo.bias', 'model.bert.encoder.layer.5.mlp.gated_layers.weight', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter.4.bias', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter.5.freq', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.10.mlp.layernorm.weight', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter.4.bias', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter.0.weight', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter.1.freq', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.2.attention.filter_fn2.modulation.deltas', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter.0.bias', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter.1.freq', 'model.bert.encoder.layer.8.attention.short_filter.weight', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.7.attention.filter_fn.modulation.deltas', 'model.bert.encoder.layer.9.mlp.layernorm.weight', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.2.mlp.wo.bias', 'model.bert.encoder.layer.6.attention.out_linear.bias', 'model.bert.encoder.layer.7.mlp.gated_layers.weight', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter.0.bias', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter.2.weight', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter.2.bias', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter.4.bias', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter.2.weight', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.4.attention.short_filter.weight', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter.0.bias', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter.4.weight', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.7.attention.short_filter.weight', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.0.attention.filter_fn.bias', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.11.mlp.layernorm.bias', 'model.bert.encoder.layer.6.attention.filter_fn.pos_emb.z', 'model.bert.encoder.layer.7.attention.out_linear.bias', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.9.attention.out_linear.weight', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter.2.weight', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.9.attention.filter_fn.bias', 'model.bert.encoder.layer.11.attention.filter_fn.modulation.deltas', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter.5.freq', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter.2.bias', 'model.bert.encoder.layer.10.attention.out_linear.bias', 'model.bert.encoder.layer.0.attention.filter_fn.pos_emb.z', 'model.bert.encoder.layer.11.attention.in_linear.weight', 'model.bert.encoder.layer.11.mlp.wo.bias', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter.0.weight', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter_rev.0.weight', 'model.bert.embeddings.position_ids', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.5.attention.filter_fn.pos_emb.t', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter.0.bias', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter.4.weight', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter.0.bias', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter.4.weight', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter.0.bias', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.6.mlp.wo.bias', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.7.attention.in_linear.weight', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter.2.weight', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter.0.bias', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter.4.bias', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter.2.bias', 'model.bert.encoder.layer.4.attention.filter_fn2.bias', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter.0.weight', 'model.cls.predictions.decoder.weight', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter.2.bias', 'model.bert.encoder.layer.8.attention.short_filter.bias', 'model.bert.encoder.layer.8.attention.in_linear.bias', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter.2.weight', 'model.bert.encoder.layer.8.attention.filter_fn.pos_emb.z', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter.3.freq', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter.6.weight', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.10.attention.filter_fn2.bias', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter.1.freq', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter.4.weight', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter.0.weight', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter.4.weight', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter.6.weight', 'model.bert.encoder.layer.2.mlp.layernorm.bias', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter.1.freq', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.6.attention.filter_fn2.pos_emb.t', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter.3.freq', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter.0.weight', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter.0.bias', 'model.bert.encoder.layer.9.attention.in_linear.weight', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter.1.freq', 'model.bert.encoder.layer.3.mlp.gated_layers.weight', 'model.bert.encoder.layer.4.attention.filter_fn.pos_emb.z', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter.0.bias', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.6.attention.filter_fn.modulation.deltas', 'model.cls.predictions.decoder.bias', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.8.attention.filter_fn2.bias', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter.4.weight', 'model.bert.encoder.layer.2.attention.out_linear.weight', 'model.bert.encoder.layer.6.attention.short_filter.bias', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter.2.weight', 'model.bert.encoder.layer.10.attention.out_linear.weight', 'model.bert.encoder.layer.4.attention.filter_fn.modulation.deltas', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.8.mlp.layernorm.weight', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.1.mlp.gated_layers.weight', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter.2.weight', 'model.bert.encoder.layer.10.mlp.gated_layers.weight', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter.4.bias', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter.4.bias', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter.2.bias', 'model.bert.encoder.layer.6.mlp.wo.weight', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter.5.freq', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter.4.weight', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter.4.bias', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.11.attention.filter_fn2.pos_emb.t', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.9.mlp.gated_layers.weight', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter.1.freq', 'model.bert.encoder.layer.1.attention.out_linear.bias', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter.2.weight', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.5.attention.in_linear.weight', 'model.bert.encoder.layer.4.attention.in_linear.bias', 'model.bert.encoder.layer.3.attention.filter_fn.bias', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter.3.freq', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter.0.weight', 'model.bert.encoder.layer.5.mlp.layernorm.weight', 'model.bert.encoder.layer.5.attention.in_linear.bias', 'model.bert.encoder.layer.11.mlp.gated_layers.weight', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter.1.freq', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.2.attention.filter_fn.modulation.deltas', 'model.bert.encoder.layer.4.mlp.wo.weight', 'model.bert.encoder.layer.0.attention.out_linear.weight', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.7.attention.short_filter.bias', 'model.bert.encoder.layer.3.attention.filter_fn2.modulation.deltas', 'model.bert.encoder.layer.6.attention.filter_fn2.bias', 'model.bert.encoder.layer.7.attention.in_linear.bias', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter.3.freq', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter.6.weight', 'model.bert.encoder.layer.2.attention.short_filter.bias', 'model.bert.encoder.layer.5.attention.filter_fn.pos_emb.z', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.8.mlp.wo.bias', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter.2.weight', 'model.bert.encoder.layer.9.mlp.wo.bias', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter.4.weight', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter.2.weight', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter.5.freq', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.2.attention.out_linear.bias', 'model.bert.encoder.layer.9.attention.out_linear.bias', 'model.bert.encoder.layer.9.attention.in_linear.bias', 'model.bert.encoder.layer.9.attention.filter_fn2.modulation.deltas', 'model.bert.encoder.layer.1.attention.filter_fn.pos_emb.t', 'model.bert.encoder.layer.0.attention.filter_fn2.pos_emb.z', 'model.bert.encoder.layer.4.mlp.layernorm.bias', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter.3.freq', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter.4.weight', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter.2.bias', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.6.attention.in_linear.weight', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter.2.weight', 'model.bert.encoder.layer.10.attention.filter_fn2.modulation.deltas', 'model.bert.encoder.layer.11.attention.filter_fn.pos_emb.z', 'model.bert.encoder.layer.3.mlp.wo.weight', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter.4.bias', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter.4.weight', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter.3.freq', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.1.attention.in_linear.bias', 'model.bert.encoder.layer.5.attention.short_filter.bias', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.5.attention.short_filter.weight', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter.1.freq', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.9.attention.filter_fn2.bias', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter.5.freq', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter.1.freq', 'model.bert.encoder.layer.11.attention.short_filter.bias', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter.2.bias', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter_rev.3.freq', 'model.cls.predictions.transform.LayerNorm.bias', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter.2.bias', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter.6.weight', 'model.bert.encoder.layer.10.attention.filter_fn.pos_emb.t', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter.0.bias', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.9.attention.filter_fn.modulation.deltas', 'model.bert.encoder.layer.9.attention.filter_fn.pos_emb.t', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter.2.bias', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.1.mlp.wo.bias', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.0.attention.out_linear.bias', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter.0.bias', 'model.bert.encoder.layer.1.attention.filter_fn2.pos_emb.t', 'model.bert.encoder.layer.1.mlp.layernorm.bias', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter.2.weight', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter.1.freq', 'model.bert.encoder.layer.4.attention.short_filter.bias', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter.0.weight', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter.0.weight', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter.0.bias', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter.4.bias', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.0.attention.short_filter.bias', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter.1.freq', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter.2.bias', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter.0.weight', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter.0.weight', 'model.bert.encoder.layer.3.attention.filter_fn2.pos_emb.z', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter.3.freq', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter.0.weight', 'model.bert.encoder.layer.8.mlp.gated_layers.weight', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.10.attention.filter_fn.implicit_filter.0.weight', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter.2.weight', 'model.bert.encoder.layer.4.attention.filter_fn2.modulation.deltas', 'model.bert.encoder.layer.6.attention.short_filter.weight', 'model.bert.encoder.layer.11.attention.filter_fn.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.8.mlp.wo.weight', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter.6.weight', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter.4.bias', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter.1.freq', 'model.bert.encoder.layer.5.attention.filter_fn.bias', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter.2.bias', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter.3.freq', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter.5.freq', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter.2.weight', 'model.bert.encoder.layer.4.mlp.wo.bias', 'model.bert.encoder.layer.3.attention.out_linear.bias', 'model.bert.encoder.layer.11.attention.filter_fn2.implicit_filter.0.bias', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter.0.bias', 'model.bert.encoder.layer.8.attention.filter_fn2.modulation.deltas', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter.6.weight', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter.1.freq', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter_rev.4.weight', 'model.bert.encoder.layer.7.attention.filter_fn.pos_emb.z', 'model.bert.encoder.layer.7.mlp.layernorm.bias', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter.0.bias', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter.1.freq', 'model.bert.encoder.layer.3.mlp.layernorm.bias', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter.2.weight', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter.4.weight', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter.4.weight', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter.2.bias', 'model.bert.encoder.layer.1.attention.filter_fn2.pos_emb.z', 'model.bert.encoder.layer.2.attention.filter_fn.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter.5.freq', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.9.mlp.layernorm.bias', 'model.bert.encoder.layer.8.attention.filter_fn.implicit_filter.0.weight', 'model.bert.encoder.layer.9.attention.short_filter.bias', 'model.bert.encoder.layer.1.attention.short_filter.weight', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.4.attention.filter_fn2.pos_emb.t', 'model.bert.encoder.layer.8.attention.out_linear.bias', 'model.bert.encoder.layer.0.attention.short_filter.weight', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter.3.freq', 'model.bert.encoder.layer.2.attention.filter_fn2.implicit_filter.0.weight', 'model.bert.encoder.layer.11.attention.out_linear.bias', 'model.bert.encoder.layer.2.attention.filter_fn.bias', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter.3.freq', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter.1.freq', 'model.bert.encoder.layer.0.attention.filter_fn.modulation.deltas', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter.0.weight', 'model.bert.encoder.layer.5.mlp.layernorm.bias', 'model.bert.encoder.layer.3.attention.filter_fn.modulation.deltas', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter.2.weight', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.6.attention.filter_fn2.pos_emb.z', 'model.bert.encoder.layer.8.attention.filter_fn2.implicit_filter.0.weight', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter.0.bias', 'model.bert.encoder.layer.0.mlp.wo.weight', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter.0.weight', 'model.bert.encoder.layer.11.attention.filter_fn.bias', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter.4.bias', 'model.bert.encoder.layer.1.attention.filter_fn.bias', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter.4.weight', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter.0.bias', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter.3.freq', 'model.bert.encoder.layer.7.attention.filter_fn.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.10.mlp.layernorm.bias', 'model.bert.encoder.layer.7.mlp.wo.bias', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.2.attention.in_linear.bias', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter_rev.2.bias', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter_rev.3.freq', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter_rev.6.weight', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter.5.freq', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter.1.freq', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.4.attention.out_linear.bias', 'model.bert.encoder.layer.4.attention.filter_fn.bias', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter.0.weight', 'model.bert.encoder.layer.1.attention.filter_fn2.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.9.attention.filter_fn2.implicit_filter.5.freq', 'model.bert.encoder.layer.8.attention.filter_fn.pos_emb.t', 'model.bert.encoder.layer.7.attention.out_linear.weight', 'model.bert.encoder.layer.1.attention.filter_fn.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.3.attention.filter_fn.pos_emb.z', 'model.bert.encoder.layer.5.attention.out_linear.weight', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.5.mlp.wo.bias', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter.2.bias', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter.0.bias', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.5.attention.filter_fn2.pos_emb.z', 'model.bert.encoder.layer.5.attention.filter_fn.implicit_filter_rev.4.bias', 'model.bert.encoder.layer.6.attention.filter_fn.implicit_filter.3.freq', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.4.attention.filter_fn.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.2.mlp.wo.weight', 'model.bert.encoder.layer.10.attention.filter_fn2.implicit_filter.4.bias', 'model.bert.encoder.layer.7.attention.filter_fn2.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.10.attention.filter_fn2.pos_emb.z', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter_rev.0.bias', 'model.bert.encoder.layer.0.attention.filter_fn.implicit_filter.2.weight', 'model.bert.encoder.layer.0.attention.filter_fn2.implicit_filter.4.weight', 'model.bert.encoder.layer.11.attention.short_filter.weight', 'model.bert.encoder.layer.3.attention.filter_fn.implicit_filter_rev.2.weight', 'model.bert.encoder.layer.6.attention.filter_fn2.implicit_filter.6.weight', 'model.bert.encoder.layer.0.attention.in_linear.bias', 'model.bert.encoder.layer.11.mlp.layernorm.weight', 'model.bert.embeddings.word_embeddings.weight', 'model.bert.encoder.layer.4.attention.filter_fn2.pos_emb.z', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter.4.weight', 'model.bert.encoder.layer.9.attention.filter_fn.implicit_filter.3.freq', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter_rev.0.weight', 'model.bert.encoder.layer.5.attention.filter_fn2.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.4.attention.filter_fn2.implicit_filter_rev.1.freq', 'model.bert.encoder.layer.3.attention.filter_fn2.implicit_filter_rev.5.freq', 'model.bert.encoder.layer.7.attention.filter_fn2.pos_emb.t']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at togethercomputer/m2-bert-80M-2k and are newly initialized: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer, AutoModelForMaskedLM, BertConfig, BertModel, BertForPreTraining, BertForMaskedLM, AutoModel, PretrainedConfig, AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from collate import DataCollatorForLanguageModelingSpan\n",
    "\n",
    "\n",
    "dataset = load_from_disk(\"../batch128\")\n",
    "dataset = dataset.remove_columns([\"species_name\", \"__index_level_0__\"])\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gagneurlab/SpeciesLM\", revision=\"downstream_species_lm\")\n",
    "\n",
    "# This way we don't load weights\n",
    "# https://stackoverflow.com/questions/65072694/make-sure-bert-model-does-not-load-pretrained-weights\n",
    "# TODO AutConfig or AutoModel? i guess it doesn't matter\n",
    "\n",
    "#config = PretrainedConfig.from_pretrained(\"togethercomputer/m2-bert-80M-2k\")\n",
    "#model = BertForMaskedLM(config)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "  \"togethercomputer/m2-bert-80M-2k\",\n",
    "  trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30528, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(2048, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): MonarchMixerSequenceMixing(\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (filter_fn2): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): BlockdiagLinear()\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): BlockdiagLinear()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): MonarchMixerSequenceMixing(\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (filter_fn2): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): BlockdiagLinear()\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): BlockdiagLinear()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): MonarchMixerSequenceMixing(\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (filter_fn2): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): BlockdiagLinear()\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): BlockdiagLinear()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): MonarchMixerSequenceMixing(\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (filter_fn2): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): BlockdiagLinear()\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): BlockdiagLinear()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): MonarchMixerSequenceMixing(\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (filter_fn2): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): BlockdiagLinear()\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): BlockdiagLinear()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): MonarchMixerSequenceMixing(\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (filter_fn2): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): BlockdiagLinear()\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): BlockdiagLinear()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): MonarchMixerSequenceMixing(\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (filter_fn2): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): BlockdiagLinear()\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): BlockdiagLinear()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): MonarchMixerSequenceMixing(\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (filter_fn2): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): BlockdiagLinear()\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): BlockdiagLinear()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): MonarchMixerSequenceMixing(\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (filter_fn2): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): BlockdiagLinear()\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): BlockdiagLinear()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): MonarchMixerSequenceMixing(\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (filter_fn2): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): BlockdiagLinear()\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): BlockdiagLinear()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): MonarchMixerSequenceMixing(\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (filter_fn2): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): BlockdiagLinear()\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): BlockdiagLinear()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): MonarchMixerSequenceMixing(\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (filter_fn2): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (implicit_filter_rev): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "            (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): BlockdiagLinear()\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): BlockdiagLinear()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30528, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to reinitialize the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0056, -0.0145, -0.0089, -0.0047, -0.0043],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0164,  0.0040,  0.0007]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.bert.encoder.layer[0].attention.filter_fn.implicit_filter[0].weight[0])\n",
    "print(model.bert.encoder.layer[0].attention.short_filter.weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "modules = model.named_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0075, -0.0093,  0.0100, -0.0018,  0.0285],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.0049, -0.0237,  0.0216]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# using this function from stackoverflow\n",
    "# added randomizing convolutions\n",
    "def randomize_model(model):\n",
    "    for module_ in model.named_modules(): \n",
    "        if isinstance(module_[1],(torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv1d)):\n",
    "            module_[1].weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n",
    "        elif isinstance(module_[1], torch.nn.LayerNorm):\n",
    "            module_[1].bias.data.zero_()\n",
    "            module_[1].weight.data.fill_(1.0)\n",
    "        if isinstance(module_[1], (torch.nn.Linear, torch.nn.Conv1d)) and module_[1].bias is not None:\n",
    "            module_[1].bias.data.zero_()\n",
    "    return model\n",
    "\n",
    "randomize_model(model)\n",
    "\n",
    "print(model.bert.encoder.layer[0].attention.filter_fn.implicit_filter[0].weight[0])\n",
    "print(model.bert.encoder.layer[0].attention.short_filter.weight[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Trying out if we still get the loss to go to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=singlesamplednam2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 37\u001b[0m\n\u001b[1;32m      9\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     10\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/correct_model_1sample\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#\"wandb\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     30\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     31\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator\n\u001b[1;32m     35\u001b[0m )\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1927\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1928\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1929\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1932\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1933\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1934\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1935\u001b[0m ):\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py:2699\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2698\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2699\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2702\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py:2731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2729\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2730\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2731\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2732\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/togethercomputer/m2-bert-80M-2k/3e061b6d18dd7fab5df5047432edca801fe89e5d/bert_layers.py:718\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m#print(\"MLM Outputs\")\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m#print(outputs[0].shape)\u001b[39;00m\n\u001b[1;32m    716\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 718\u001b[0m last_hidden_state_formatted \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m: last_hidden_state_formatted}\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "#Load model directly\n",
    "%env WANDB_PROJECT=singlesamplednam2\n",
    "\n",
    "dataset = load_from_disk(\"../microset\")\n",
    "dataset = dataset.remove_columns([\"species_name\", \"__index_level_0__\"])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModelingSpan(tokenizer, mlm=True, mlm_probability = 0.02, span_length = 6)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/correct_model_1sample\",\n",
    "    \n",
    "    max_steps=20000,\n",
    "    \n",
    "    seed=17,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=16,\n",
    "    \n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    \n",
    "    evaluation_strategy=\"no\",\n",
    "    \n",
    "    #dataloader_num_workers=4,\n",
    "    #dataloader_prefetch_factor=2,\n",
    "    run_name=\"correct_model_1sample\",\n",
    "    report_to=\"none\" #\"wandb\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [tensor([2]),\n",
       "  tensor([4837]),\n",
       "  tensor([1092]),\n",
       "  tensor([260]),\n",
       "  tensor([1025]),\n",
       "  tensor([4085]),\n",
       "  tensor([4037]),\n",
       "  tensor([3847]),\n",
       "  tensor([3086]),\n",
       "  tensor([44]),\n",
       "  tensor([161]),\n",
       "  tensor([631]),\n",
       "  tensor([2509]),\n",
       "  tensor([1829]),\n",
       "  tensor([3207]),\n",
       "  tensor([526]),\n",
       "  tensor([2090]),\n",
       "  tensor([153]),\n",
       "  tensor([598]),\n",
       "  tensor([2379]),\n",
       "  tensor([1309]),\n",
       "  tensor([1128]),\n",
       "  tensor([401]),\n",
       "  tensor([1592]),\n",
       "  tensor([2257]),\n",
       "  tensor([822]),\n",
       "  tensor([3276]),\n",
       "  tensor([801]),\n",
       "  tensor([3190]),\n",
       "  tensor([458]),\n",
       "  tensor([1820]),\n",
       "  tensor([3170]),\n",
       "  tensor([379]),\n",
       "  tensor([1503]),\n",
       "  tensor([1901]),\n",
       "  tensor([3496]),\n",
       "  tensor([1684]),\n",
       "  tensor([2626]),\n",
       "  tensor([2299]),\n",
       "  tensor([992]),\n",
       "  tensor([3955]),\n",
       "  tensor([3518]),\n",
       "  tensor([1772]),\n",
       "  tensor([2977]),\n",
       "  tensor([3701]),\n",
       "  tensor([2501]),\n",
       "  tensor([1800]),\n",
       "  tensor([3091]),\n",
       "  tensor([64]),\n",
       "  tensor([241]),\n",
       "  tensor([949]),\n",
       "  tensor([3783]),\n",
       "  tensor([2831]),\n",
       "  tensor([3120]),\n",
       "  tensor([179]),\n",
       "  tensor([702]),\n",
       "  tensor([2794]),\n",
       "  tensor([2971]),\n",
       "  tensor([3677]),\n",
       "  tensor([2407]),\n",
       "  tensor([1421]),\n",
       "  tensor([1575]),\n",
       "  tensor([2189]),\n",
       "  tensor([550]),\n",
       "  tensor([2187]),\n",
       "  tensor([543]),\n",
       "  tensor([2158]),\n",
       "  tensor([426]),\n",
       "  tensor([1690]),\n",
       "  tensor([2650]),\n",
       "  tensor([2393]),\n",
       "  tensor([1367]),\n",
       "  tensor([1357]),\n",
       "  tensor([1319]),\n",
       "  tensor([1168]),\n",
       "  tensor([563]),\n",
       "  tensor([2238]),\n",
       "  tensor([748]),\n",
       "  tensor([2979]),\n",
       "  tensor([3710]),\n",
       "  tensor([2540]),\n",
       "  tensor([1955]),\n",
       "  tensor([3709]),\n",
       "  tensor([2536]),\n",
       "  tensor([1937]),\n",
       "  tensor([3637]),\n",
       "  tensor([2248]),\n",
       "  tensor([787]),\n",
       "  tensor([3135]),\n",
       "  tensor([238]),\n",
       "  tensor([939]),\n",
       "  tensor([3741]),\n",
       "  tensor([2662]),\n",
       "  tensor([2442]),\n",
       "  tensor([1564]),\n",
       "  tensor([2145]),\n",
       "  tensor([374]),\n",
       "  tensor([1483]),\n",
       "  tensor([1821]),\n",
       "  tensor([3175]),\n",
       "  tensor([398]),\n",
       "  tensor([1578]),\n",
       "  tensor([2202]),\n",
       "  tensor([604]),\n",
       "  tensor([2404]),\n",
       "  tensor([1409]),\n",
       "  tensor([1527]),\n",
       "  tensor([1999]),\n",
       "  tensor([3885]),\n",
       "  tensor([3237]),\n",
       "  tensor([648]),\n",
       "  tensor([2578]),\n",
       "  tensor([2105]),\n",
       "  tensor([214]),\n",
       "  tensor([841]),\n",
       "  tensor([3351]),\n",
       "  tensor([1101]),\n",
       "  tensor([296]),\n",
       "  tensor([1169]),\n",
       "  tensor([565]),\n",
       "  tensor([2247]),\n",
       "  tensor([783]),\n",
       "  tensor([3120]),\n",
       "  tensor([179]),\n",
       "  tensor([703]),\n",
       "  tensor([2797]),\n",
       "  tensor([2982]),\n",
       "  tensor([3723]),\n",
       "  tensor([2589]),\n",
       "  tensor([2152]),\n",
       "  tensor([404]),\n",
       "  tensor([1602]),\n",
       "  tensor([2299]),\n",
       "  tensor([991]),\n",
       "  tensor([3952]),\n",
       "  tensor([3507]),\n",
       "  tensor([1727]),\n",
       "  tensor([2798]),\n",
       "  tensor([2987]),\n",
       "  tensor([3742]),\n",
       "  tensor([2668]),\n",
       "  tensor([2465]),\n",
       "  tensor([1653]),\n",
       "  tensor([2503]),\n",
       "  tensor([1808]),\n",
       "  tensor([3123]),\n",
       "  tensor([191]),\n",
       "  tensor([749]),\n",
       "  tensor([2982]),\n",
       "  tensor([3721]),\n",
       "  tensor([2582]),\n",
       "  tensor([2124]),\n",
       "  tensor([290]),\n",
       "  tensor([1145]),\n",
       "  tensor([471]),\n",
       "  tensor([1872]),\n",
       "  tensor([3377]),\n",
       "  tensor([1206]),\n",
       "  tensor([713]),\n",
       "  tensor([2838]),\n",
       "  tensor([3148]),\n",
       "  tensor([290]),\n",
       "  tensor([1146]),\n",
       "  tensor([473]),\n",
       "  tensor([1878]),\n",
       "  tensor([3402]),\n",
       "  tensor([1307]),\n",
       "  tensor([1118]),\n",
       "  tensor([361]),\n",
       "  tensor([1431]),\n",
       "  tensor([1613]),\n",
       "  tensor([2342]),\n",
       "  tensor([1163]),\n",
       "  tensor([543]),\n",
       "  tensor([2160]),\n",
       "  tensor([435]),\n",
       "  tensor([1726]),\n",
       "  tensor([2793]),\n",
       "  tensor([2967]),\n",
       "  tensor([3662]),\n",
       "  tensor([2345]),\n",
       "  tensor([1175]),\n",
       "  tensor([590]),\n",
       "  tensor([2345]),\n",
       "  tensor([1174]),\n",
       "  tensor([588]),\n",
       "  tensor([2339]),\n",
       "  tensor([1152]),\n",
       "  tensor([497]),\n",
       "  tensor([1974]),\n",
       "  tensor([3788]),\n",
       "  tensor([2850]),\n",
       "  tensor([3195]),\n",
       "  tensor([477]),\n",
       "  tensor([1895]),\n",
       "  tensor([3470]),\n",
       "  tensor([1577]),\n",
       "  tensor([2198]),\n",
       "  tensor([585]),\n",
       "  tensor([2327]),\n",
       "  tensor([1103]),\n",
       "  tensor([301]),\n",
       "  tensor([1192]),\n",
       "  tensor([659]),\n",
       "  tensor([2621]),\n",
       "  tensor([2277]),\n",
       "  tensor([903]),\n",
       "  tensor([3599]),\n",
       "  tensor([2096]),\n",
       "  tensor([180]),\n",
       "  tensor([705]),\n",
       "  tensor([2808]),\n",
       "  tensor([3025]),\n",
       "  tensor([3893]),\n",
       "  tensor([3270]),\n",
       "  tensor([780]),\n",
       "  tensor([3105]),\n",
       "  tensor([117]),\n",
       "  tensor([456]),\n",
       "  tensor([1809]),\n",
       "  tensor([3127]),\n",
       "  tensor([205]),\n",
       "  tensor([808]),\n",
       "  tensor([3217]),\n",
       "  tensor([568]),\n",
       "  tensor([2259]),\n",
       "  tensor([829]),\n",
       "  tensor([3304]),\n",
       "  tensor([914]),\n",
       "  tensor([3643]),\n",
       "  tensor([2272]),\n",
       "  tensor([883]),\n",
       "  tensor([3517]),\n",
       "  tensor([1767]),\n",
       "  tensor([2958]),\n",
       "  tensor([3625]),\n",
       "  tensor([2198]),\n",
       "  tensor([588]),\n",
       "  tensor([2340]),\n",
       "  tensor([1156]),\n",
       "  tensor([513]),\n",
       "  tensor([2037]),\n",
       "  tensor([4037]),\n",
       "  tensor([3848]),\n",
       "  tensor([3091]),\n",
       "  tensor([64]),\n",
       "  tensor([243]),\n",
       "  tensor([958]),\n",
       "  tensor([3819]),\n",
       "  tensor([2974]),\n",
       "  tensor([3692]),\n",
       "  tensor([2465]),\n",
       "  tensor([1656]),\n",
       "  tensor([2513]),\n",
       "  tensor([1848]),\n",
       "  tensor([3281]),\n",
       "  tensor([823]),\n",
       "  tensor([3280]),\n",
       "  tensor([817]),\n",
       "  tensor([3254]),\n",
       "  tensor([715]),\n",
       "  tensor([2846]),\n",
       "  tensor([3178]),\n",
       "  tensor([411]),\n",
       "  tensor([1631]),\n",
       "  tensor([2415]),\n",
       "  tensor([1455]),\n",
       "  tensor([1712]),\n",
       "  tensor([2737]),\n",
       "  tensor([2742]),\n",
       "  tensor([2763]),\n",
       "  tensor([2845]),\n",
       "  tensor([3174]),\n",
       "  tensor([395]),\n",
       "  tensor([1566]),\n",
       "  tensor([2155]),\n",
       "  tensor([414]),\n",
       "  tensor([1643]),\n",
       "  tensor([2461]),\n",
       "  tensor([1637]),\n",
       "  tensor([2439]),\n",
       "  tensor([1552]),\n",
       "  tensor([2098]),\n",
       "  tensor([187]),\n",
       "  tensor([736]),\n",
       "  tensor([2929]),\n",
       "  tensor([3509]),\n",
       "  tensor([1736]),\n",
       "  tensor([2835]),\n",
       "  tensor([3133]),\n",
       "  tensor([229]),\n",
       "  tensor([903]),\n",
       "  tensor([3599]),\n",
       "  tensor([2095]),\n",
       "  tensor([174]),\n",
       "  tensor([683]),\n",
       "  tensor([2719]),\n",
       "  tensor([3])],\n",
       " 'token_type_ids': [tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0]),\n",
       "  tensor([0])],\n",
       " 'attention_mask': [tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1])]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(dataset[\"train\"], batch_size=1, shuffle=True)\n",
    "sample = next(iter(train_dataloader))\n",
    "sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda-m2-mixer]",
   "language": "python",
   "name": "conda-env-anaconda-m2-mixer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
