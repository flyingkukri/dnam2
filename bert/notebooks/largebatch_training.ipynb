{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not import FlashFFTConv!\n",
      "20000ba\n",
      "Training using config: \n",
      "data_local: ../batchlarge/\n",
      "data_remote: null\n",
      "max_seq_len: 512\n",
      "tokenizer_name: gagneurlab/SpeciesLM\n",
      "mlm_probability: 0.02\n",
      "run_name: batchlarge_smelllr_dna_monarch-mixer-pretrain-786dim-80m-parameters2\n",
      "model:\n",
      "  name: bert\n",
      "  pretrained_model_name: bert-base-uncased\n",
      "  tokenizer_name: ${tokenizer_name}\n",
      "  model_config:\n",
      "    num_attention_heads: 12\n",
      "    num_hidden_layers: 12\n",
      "    attention_probs_dropout_prob: 0.0\n",
      "    max_position_embeddings: ${max_seq_len}\n",
      "    monarch_mixer_sequence_mixing: true\n",
      "    long_conv_l_max: ${max_seq_len}\n",
      "    long_conv_kernel_learning_rate: 0.001\n",
      "    hyena_lr_pos_emb: 1.0e-05\n",
      "    hyena_w: 10\n",
      "    hyena_wd: 0.1\n",
      "    hyena_emb_dim: 5\n",
      "    hyena_filter_order: 128\n",
      "    bidirectional: true\n",
      "    residual_long_conv: true\n",
      "    use_glu_mlp: true\n",
      "    use_monarch_mlp: true\n",
      "    monarch_mlp_nblocks: 4\n",
      "    use_positional_encodings: true\n",
      "train_loader:\n",
      "  name: dna\n",
      "  dataset:\n",
      "    local: ${data_local}\n",
      "    remote: null\n",
      "    split: train\n",
      "    tokenizer_name: ${tokenizer_name}\n",
      "    max_seq_len: ${max_seq_len}\n",
      "    shuffle: true\n",
      "    mlm_probability: ${mlm_probability}\n",
      "  drop_last: true\n",
      "  num_workers: 8\n",
      "eval_loader:\n",
      "  name: dna\n",
      "  dataset:\n",
      "    local: ${data_local}\n",
      "    remote: null\n",
      "    split: test\n",
      "    tokenizer_name: ${tokenizer_name}\n",
      "    max_seq_len: ${max_seq_len}\n",
      "    shuffle: false\n",
      "    mlm_probability: 0.15\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "scheduler:\n",
      "  name: linear_decay_with_warmup\n",
      "  t_warmup: 0.0005dur\n",
      "  alpha_f: 0.02\n",
      "precision: fp32\n",
      "optimizer:\n",
      "  name: decoupled_adamw\n",
      "  lr: 5.0e-05\n",
      "  betas:\n",
      "  - 0.99921875\n",
      "  - 0.99984375\n",
      "  eps: 1.0e-06\n",
      "  weight_decay: 1.0e-05\n",
      "algorithms:\n",
      "  gradient_clipping:\n",
      "    clipping_type: norm\n",
      "    clipping_threshold: 1.0\n",
      "max_duration: 20000ba\n",
      "eval_interval: 2000ba\n",
      "global_train_batch_size: 1024\n",
      "seed: 17\n",
      "device_eval_batch_size: 128\n",
      "device_train_microbatch_size: 16\n",
      "progress_bar: false\n",
      "log_to_console: true\n",
      "console_log_interval: 1ba\n",
      "callbacks:\n",
      "  speed_monitor:\n",
      "    window_size: 500\n",
      "  lr_monitor: {}\n",
      "loggers:\n",
      "  wandb:\n",
      "    project: singlesamplednam2\n",
      "    entity: luluh\n",
      "save_interval: 7000ba\n",
      "save_num_checkpoints_to_keep: 10\n",
      "save_folder: ./local-bert-checkpoints/${run_name}\n",
      "\n",
      "Initializing model...\n",
      "Using Monarch Mixer for Sequence Mixing: True\n",
      "-- Bidirectional: True\n",
      "-- Using Long Conv Residual: True\n",
      "-- Hyena w: 10\n",
      "-- Hyena w mod: 1\n",
      "-- Hyena filter order: 128\n",
      "-- Hyena filter dropout: 0.2\n",
      "-- Hyena filter wd: 0.1\n",
      "-- Hyena filter emb dim: 5\n",
      "-- Hyena filter lr: 0.001\n",
      "-- Hyena filter lr pos emb: 1e-05\n",
      "n_params=8.0608e+07\n",
      "Building train loader...\n",
      "==Using the correct data collator\n",
      "Building eval loader...\n",
      "==Using the correct data collator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluluhu\u001b[0m (\u001b[33mluluh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ceph/hdd/project/node_09/semi_supervised_multispecies/monarch/dnam2/bert/notebooks/wandb/run-20240225_085112-rp7t9lvh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/luluh/singlesamplednam2/runs/rp7t9lvh' target=\"_blank\">batchlarge_smelllr_dna_monarch-mixer-pretrain-786dim-80m-parameters2</a></strong> to <a href='https://wandb.ai/luluh/singlesamplednam2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/luluh/singlesamplednam2' target=\"_blank\">https://wandb.ai/luluh/singlesamplednam2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/luluh/singlesamplednam2/runs/rp7t9lvh' target=\"_blank\">https://wandb.ai/luluh/singlesamplednam2/runs/rp7t9lvh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/composer/callbacks/speed_monitor.py:120: UserWarning: gpu_flop count not found for None with precision: fp32; MFU cannot be calculated and reported. gpu_flops_available can be manuallyoverridden by setting gpu_flops_available in SpeedMonitor.\n",
      "  warnings.warn(\n",
      "******************************\n",
      "Config:\n",
      "enabled_algorithms/GradientClipping: true\n",
      "node_name: unknown because NODENAME environment variable not set\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 17\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging config...\n",
      "data_local: ../batchlarge/\n",
      "data_remote: null\n",
      "max_seq_len: 512\n",
      "tokenizer_name: gagneurlab/SpeciesLM\n",
      "mlm_probability: 0.02\n",
      "run_name: batchlarge_smelllr_dna_monarch-mixer-pretrain-786dim-80m-parameters2\n",
      "model:\n",
      "  name: bert\n",
      "  pretrained_model_name: bert-base-uncased\n",
      "  tokenizer_name: ${tokenizer_name}\n",
      "  model_config:\n",
      "    num_attention_heads: 12\n",
      "    num_hidden_layers: 12\n",
      "    attention_probs_dropout_prob: 0.0\n",
      "    max_position_embeddings: ${max_seq_len}\n",
      "    monarch_mixer_sequence_mixing: true\n",
      "    long_conv_l_max: ${max_seq_len}\n",
      "    long_conv_kernel_learning_rate: 0.001\n",
      "    hyena_lr_pos_emb: 1.0e-05\n",
      "    hyena_w: 10\n",
      "    hyena_wd: 0.1\n",
      "    hyena_emb_dim: 5\n",
      "    hyena_filter_order: 128\n",
      "    bidirectional: true\n",
      "    residual_long_conv: true\n",
      "    use_glu_mlp: true\n",
      "    use_monarch_mlp: true\n",
      "    monarch_mlp_nblocks: 4\n",
      "    use_positional_encodings: true\n",
      "train_loader:\n",
      "  name: dna\n",
      "  dataset:\n",
      "    local: ${data_local}\n",
      "    remote: null\n",
      "    split: train\n",
      "    tokenizer_name: ${tokenizer_name}\n",
      "    max_seq_len: ${max_seq_len}\n",
      "    shuffle: true\n",
      "    mlm_probability: ${mlm_probability}\n",
      "  drop_last: true\n",
      "  num_workers: 8\n",
      "eval_loader:\n",
      "  name: dna\n",
      "  dataset:\n",
      "    local: ${data_local}\n",
      "    remote: null\n",
      "    split: test\n",
      "    tokenizer_name: ${tokenizer_name}\n",
      "    max_seq_len: ${max_seq_len}\n",
      "    shuffle: false\n",
      "    mlm_probability: 0.15\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "scheduler:\n",
      "  name: linear_decay_with_warmup\n",
      "  t_warmup: 0.0005dur\n",
      "  alpha_f: 0.02\n",
      "precision: fp32\n",
      "optimizer:\n",
      "  name: decoupled_adamw\n",
      "  lr: 5.0e-05\n",
      "  betas:\n",
      "  - 0.99921875\n",
      "  - 0.99984375\n",
      "  eps: 1.0e-06\n",
      "  weight_decay: 1.0e-05\n",
      "algorithms:\n",
      "  gradient_clipping:\n",
      "    clipping_type: norm\n",
      "    clipping_threshold: 1.0\n",
      "max_duration: 20000ba\n",
      "eval_interval: 2000ba\n",
      "global_train_batch_size: 1024\n",
      "seed: 17\n",
      "device_eval_batch_size: 128\n",
      "device_train_microbatch_size: 16\n",
      "progress_bar: false\n",
      "log_to_console: true\n",
      "console_log_interval: 1ba\n",
      "callbacks:\n",
      "  speed_monitor:\n",
      "    window_size: 500\n",
      "  lr_monitor: {}\n",
      "loggers:\n",
      "  wandb:\n",
      "    project: singlesamplednam2\n",
      "    entity: luluh\n",
      "save_interval: 7000ba\n",
      "save_num_checkpoints_to_keep: 10\n",
      "save_folder: ./local-bert-checkpoints/${run_name}\n",
      "n_gpus: 1\n",
      "device_train_batch_size: 1024\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[batch=1/20000]:\n",
      "\t Train time/epoch: 0\n",
      "\t Train time/batch: 0\n",
      "\t Train time/sample: 0\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 0\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 10.4706\n",
      "\t Train metrics/train/LanguageCrossEntropy: 10.4708\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0001\n",
      "\t Train time/train: 0.0065\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0065\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=2/20000]:\n",
      "\t Train time/epoch: 1\n",
      "\t Train time/batch: 1\n",
      "\t Train time/sample: 1024\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 307200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 10.4719\n",
      "\t Train metrics/train/LanguageCrossEntropy: 10.4718\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0001\n",
      "\t Train time/train: 0.0116\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0116\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=3/20000]:\n",
      "\t Train time/epoch: 2\n",
      "\t Train time/batch: 2\n",
      "\t Train time/sample: 2048\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 614400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 10.4681\n",
      "\t Train metrics/train/LanguageCrossEntropy: 10.4689\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0001\n",
      "\t Train time/train: 0.0166\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0166\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=4/20000]:\n",
      "\t Train time/epoch: 3\n",
      "\t Train time/batch: 3\n",
      "\t Train time/sample: 3072\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 921600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 10.4449\n",
      "\t Train metrics/train/LanguageCrossEntropy: 10.4449\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0000\n",
      "\t Train time/train: 0.0216\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0216\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=5/20000]:\n",
      "\t Train time/epoch: 4\n",
      "\t Train time/batch: 4\n",
      "\t Train time/sample: 4096\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 1228800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 10.4343\n",
      "\t Train metrics/train/LanguageCrossEntropy: 10.4341\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0001\n",
      "\t Train time/train: 0.0267\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0267\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=6/20000]:\n",
      "\t Train time/epoch: 5\n",
      "\t Train time/batch: 5\n",
      "\t Train time/sample: 5120\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 1536000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 10.4130\n",
      "\t Train metrics/train/LanguageCrossEntropy: 10.4129\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0001\n",
      "\t Train time/train: 0.0317\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0317\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=7/20000]:\n",
      "\t Train time/epoch: 6\n",
      "\t Train time/batch: 6\n",
      "\t Train time/sample: 6144\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 1843200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 10.3914\n",
      "\t Train metrics/train/LanguageCrossEntropy: 10.3911\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0002\n",
      "\t Train time/train: 0.0367\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0367\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=8/20000]:\n",
      "\t Train time/epoch: 7\n",
      "\t Train time/batch: 7\n",
      "\t Train time/sample: 7168\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 2150400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 10.3635\n",
      "\t Train metrics/train/LanguageCrossEntropy: 10.3638\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0001\n",
      "\t Train time/train: 0.0417\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0417\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=9/20000]:\n",
      "\t Train time/epoch: 8\n",
      "\t Train time/batch: 8\n",
      "\t Train time/sample: 8192\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 2457600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 10.3357\n",
      "\t Train metrics/train/LanguageCrossEntropy: 10.3360\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0003\n",
      "\t Train time/train: 0.0467\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0467\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=10/20000]:\n",
      "\t Train time/epoch: 9\n",
      "\t Train time/batch: 9\n",
      "\t Train time/sample: 9216\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 2764800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 10.3024\n",
      "\t Train metrics/train/LanguageCrossEntropy: 10.3032\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.0517\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0517\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=11/20000]:\n",
      "\t Train time/epoch: 10\n",
      "\t Train time/batch: 10\n",
      "\t Train time/sample: 10240\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 3072000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 10.2752\n",
      "\t Train metrics/train/LanguageCrossEntropy: 10.2752\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.0567\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0567\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0001\n",
      "[batch=12/20000]:\n",
      "\t Train time/epoch: 11\n",
      "\t Train time/batch: 11\n",
      "\t Train time/sample: 11264\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 3379200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 10.2417\n",
      "\t Train metrics/train/LanguageCrossEntropy: 10.2417\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.0617\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0617\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=13/20000]:\n",
      "\t Train time/epoch: 12\n",
      "\t Train time/batch: 12\n",
      "\t Train time/sample: 12288\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 3686400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 10.2013\n",
      "\t Train metrics/train/LanguageCrossEntropy: 10.2013\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0003\n",
      "\t Train time/train: 0.0668\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0668\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=14/20000]:\n",
      "\t Train time/epoch: 13\n",
      "\t Train time/batch: 13\n",
      "\t Train time/sample: 13312\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 3993600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 10.1618\n",
      "\t Train metrics/train/LanguageCrossEntropy: 10.1616\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0005\n",
      "\t Train time/train: 0.0718\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0718\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=15/20000]:\n",
      "\t Train time/epoch: 14\n",
      "\t Train time/batch: 14\n",
      "\t Train time/sample: 14336\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 4300800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 10.1273\n",
      "\t Train metrics/train/LanguageCrossEntropy: 10.1272\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0002\n",
      "\t Train time/train: 0.0768\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0768\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=16/20000]:\n",
      "\t Train time/epoch: 15\n",
      "\t Train time/batch: 15\n",
      "\t Train time/sample: 15360\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 4608000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 10.0916\n",
      "\t Train metrics/train/LanguageCrossEntropy: 10.0915\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0002\n",
      "\t Train time/train: 0.0818\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0818\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=17/20000]:\n",
      "\t Train time/epoch: 16\n",
      "\t Train time/batch: 16\n",
      "\t Train time/sample: 16384\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 4915200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 10.0568\n",
      "\t Train metrics/train/LanguageCrossEntropy: 10.0564\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0006\n",
      "\t Train time/train: 0.0868\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0868\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=18/20000]:\n",
      "\t Train time/epoch: 17\n",
      "\t Train time/batch: 17\n",
      "\t Train time/sample: 17408\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 5222400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 10.0189\n",
      "\t Train metrics/train/LanguageCrossEntropy: 10.0194\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0002\n",
      "\t Train time/train: 0.0918\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0918\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=19/20000]:\n",
      "\t Train time/epoch: 18\n",
      "\t Train time/batch: 18\n",
      "\t Train time/sample: 18432\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 5529600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.9805\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.9805\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0003\n",
      "\t Train time/train: 0.0968\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0968\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=20/20000]:\n",
      "\t Train time/epoch: 19\n",
      "\t Train time/batch: 19\n",
      "\t Train time/sample: 19456\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 5836800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.9428\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.9426\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0003\n",
      "\t Train time/train: 0.1018\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1018\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=21/20000]:\n",
      "\t Train time/epoch: 20\n",
      "\t Train time/batch: 20\n",
      "\t Train time/sample: 20480\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 6144000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.9096\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.9093\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.1069\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1069\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=22/20000]:\n",
      "\t Train time/epoch: 21\n",
      "\t Train time/batch: 21\n",
      "\t Train time/sample: 21504\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 6451200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.8698\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.8701\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0002\n",
      "\t Train time/train: 0.1119\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1119\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=23/20000]:\n",
      "\t Train time/epoch: 22\n",
      "\t Train time/batch: 22\n",
      "\t Train time/sample: 22528\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 6758400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.8327\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.8326\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0005\n",
      "\t Train time/train: 0.1169\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1169\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=24/20000]:\n",
      "\t Train time/epoch: 23\n",
      "\t Train time/batch: 23\n",
      "\t Train time/sample: 23552\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 7065600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.8000\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.7996\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0003\n",
      "\t Train time/train: 0.1219\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1219\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=25/20000]:\n",
      "\t Train time/epoch: 24\n",
      "\t Train time/batch: 24\n",
      "\t Train time/sample: 24576\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 7372800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.7549\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.7547\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0003\n",
      "\t Train time/train: 0.1270\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1270\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=26/20000]:\n",
      "\t Train time/epoch: 25\n",
      "\t Train time/batch: 25\n",
      "\t Train time/sample: 25600\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 7680000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.7196\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.7198\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.1320\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1320\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=27/20000]:\n",
      "\t Train time/epoch: 26\n",
      "\t Train time/batch: 26\n",
      "\t Train time/sample: 26624\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 7987200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.6901\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.6901\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.1370\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1370\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=28/20000]:\n",
      "\t Train time/epoch: 27\n",
      "\t Train time/batch: 27\n",
      "\t Train time/sample: 27648\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 8294400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.6434\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.6430\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.1421\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1421\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=29/20000]:\n",
      "\t Train time/epoch: 28\n",
      "\t Train time/batch: 28\n",
      "\t Train time/sample: 28672\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 8601600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.6099\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.6099\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.1471\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1471\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=30/20000]:\n",
      "\t Train time/epoch: 29\n",
      "\t Train time/batch: 29\n",
      "\t Train time/sample: 29696\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 8908800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.5731\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.5735\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.1521\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1521\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=31/20000]:\n",
      "\t Train time/epoch: 30\n",
      "\t Train time/batch: 30\n",
      "\t Train time/sample: 30720\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 9216000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.5338\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.5337\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0003\n",
      "\t Train time/train: 0.1572\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1572\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=32/20000]:\n",
      "\t Train time/epoch: 31\n",
      "\t Train time/batch: 31\n",
      "\t Train time/sample: 31744\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 9523200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.4959\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.4958\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.1622\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1622\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=33/20000]:\n",
      "\t Train time/epoch: 32\n",
      "\t Train time/batch: 32\n",
      "\t Train time/sample: 32768\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 9830400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.4619\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.4613\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0006\n",
      "\t Train time/train: 0.1672\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1672\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=34/20000]:\n",
      "\t Train time/epoch: 33\n",
      "\t Train time/batch: 33\n",
      "\t Train time/sample: 33792\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 10137600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.4324\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.4324\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0003\n",
      "\t Train time/train: 0.1722\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1722\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=35/20000]:\n",
      "\t Train time/epoch: 34\n",
      "\t Train time/batch: 34\n",
      "\t Train time/sample: 34816\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 10444800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.3969\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.3961\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0002\n",
      "\t Train time/train: 0.1772\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1772\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=36/20000]:\n",
      "\t Train time/epoch: 35\n",
      "\t Train time/batch: 35\n",
      "\t Train time/sample: 35840\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 10752000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.3620\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.3624\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.1822\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1822\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=37/20000]:\n",
      "\t Train time/epoch: 36\n",
      "\t Train time/batch: 36\n",
      "\t Train time/sample: 36864\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 11059200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.3240\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.3238\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0005\n",
      "\t Train time/train: 0.1873\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1873\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=38/20000]:\n",
      "\t Train time/epoch: 37\n",
      "\t Train time/batch: 37\n",
      "\t Train time/sample: 37888\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 11366400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.2963\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.2974\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.1923\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1923\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=39/20000]:\n",
      "\t Train time/epoch: 38\n",
      "\t Train time/batch: 38\n",
      "\t Train time/sample: 38912\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 11673600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.2594\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.2591\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.1973\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.1973\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=40/20000]:\n",
      "\t Train time/epoch: 39\n",
      "\t Train time/batch: 39\n",
      "\t Train time/sample: 39936\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 11980800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.2194\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.2196\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0006\n",
      "\t Train time/train: 0.2023\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.2023\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=41/20000]:\n",
      "\t Train time/epoch: 40\n",
      "\t Train time/batch: 40\n",
      "\t Train time/sample: 40960\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 12288000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.1973\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.1979\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.2073\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.2073\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=42/20000]:\n",
      "\t Train time/epoch: 41\n",
      "\t Train time/batch: 41\n",
      "\t Train time/sample: 41984\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 12595200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.1648\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.1648\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0005\n",
      "\t Train time/train: 0.2123\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.2123\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=43/20000]:\n",
      "\t Train time/epoch: 42\n",
      "\t Train time/batch: 42\n",
      "\t Train time/sample: 43008\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 12902400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.1348\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.1345\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.2175\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.2175\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=44/20000]:\n",
      "\t Train time/epoch: 43\n",
      "\t Train time/batch: 43\n",
      "\t Train time/sample: 44032\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 13209600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.1088\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.1092\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0003\n",
      "\t Train time/train: 0.2280\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.2280\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=45/20000]:\n",
      "\t Train time/epoch: 44\n",
      "\t Train time/batch: 44\n",
      "\t Train time/sample: 45056\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 13516800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.0770\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.0775\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.2385\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.2385\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=46/20000]:\n",
      "\t Train time/epoch: 45\n",
      "\t Train time/batch: 45\n",
      "\t Train time/sample: 46080\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 13824000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.0427\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.0430\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0005\n",
      "\t Train time/train: 0.2490\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.2490\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=47/20000]:\n",
      "\t Train time/epoch: 46\n",
      "\t Train time/batch: 46\n",
      "\t Train time/sample: 47104\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 14131200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 9.0151\n",
      "\t Train metrics/train/LanguageCrossEntropy: 9.0148\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.2595\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.2595\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=48/20000]:\n",
      "\t Train time/epoch: 47\n",
      "\t Train time/batch: 47\n",
      "\t Train time/sample: 48128\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 14438400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.9871\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.9868\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.2699\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.2699\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=49/20000]:\n",
      "\t Train time/epoch: 48\n",
      "\t Train time/batch: 48\n",
      "\t Train time/sample: 49152\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 14745600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.9662\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.9662\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0006\n",
      "\t Train time/train: 0.2804\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.2804\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=50/20000]:\n",
      "\t Train time/epoch: 49\n",
      "\t Train time/batch: 49\n",
      "\t Train time/sample: 50176\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 15052800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.9399\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.9396\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0006\n",
      "\t Train time/train: 0.2908\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.2908\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=51/20000]:\n",
      "\t Train time/epoch: 50\n",
      "\t Train time/batch: 50\n",
      "\t Train time/sample: 51200\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 15360000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.9172\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.9172\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.3013\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.3013\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=52/20000]:\n",
      "\t Train time/epoch: 51\n",
      "\t Train time/batch: 51\n",
      "\t Train time/sample: 52224\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 15667200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.8884\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.8881\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.3117\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.3117\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=53/20000]:\n",
      "\t Train time/epoch: 52\n",
      "\t Train time/batch: 52\n",
      "\t Train time/sample: 53248\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 15974400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.8663\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.8666\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0007\n",
      "\t Train time/train: 0.3222\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.3222\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=54/20000]:\n",
      "\t Train time/epoch: 53\n",
      "\t Train time/batch: 53\n",
      "\t Train time/sample: 54272\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 16281600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.8498\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.8501\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0003\n",
      "\t Train time/train: 0.3326\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.3326\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=55/20000]:\n",
      "\t Train time/epoch: 54\n",
      "\t Train time/batch: 54\n",
      "\t Train time/sample: 55296\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 16588800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.8165\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.8167\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0006\n",
      "\t Train time/train: 0.3431\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.3431\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=56/20000]:\n",
      "\t Train time/epoch: 55\n",
      "\t Train time/batch: 55\n",
      "\t Train time/sample: 56320\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 16896000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.7936\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.7935\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0005\n",
      "\t Train time/train: 0.3536\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.3536\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=57/20000]:\n",
      "\t Train time/epoch: 56\n",
      "\t Train time/batch: 56\n",
      "\t Train time/sample: 57344\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 17203200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.7789\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.7790\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 0.3640\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.3640\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=58/20000]:\n",
      "\t Train time/epoch: 57\n",
      "\t Train time/batch: 57\n",
      "\t Train time/sample: 58368\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 17510400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.7545\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.7549\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0005\n",
      "\t Train time/train: 0.3745\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.3745\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=59/20000]:\n",
      "\t Train time/epoch: 58\n",
      "\t Train time/batch: 58\n",
      "\t Train time/sample: 59392\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 17817600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.7377\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.7380\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0006\n",
      "\t Train time/train: 0.3850\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.3850\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=60/20000]:\n",
      "\t Train time/epoch: 59\n",
      "\t Train time/batch: 59\n",
      "\t Train time/sample: 60416\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 18124800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.7122\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.7123\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0006\n",
      "\t Train time/train: 0.3955\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.3955\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=61/20000]:\n",
      "\t Train time/epoch: 60\n",
      "\t Train time/batch: 60\n",
      "\t Train time/sample: 61440\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 18432000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.6943\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.6946\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0005\n",
      "\t Train time/train: 0.4059\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.4059\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=62/20000]:\n",
      "\t Train time/epoch: 61\n",
      "\t Train time/batch: 61\n",
      "\t Train time/sample: 62464\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 18739200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.6804\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.6802\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0005\n",
      "\t Train time/train: 0.4164\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.4164\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=63/20000]:\n",
      "\t Train time/epoch: 62\n",
      "\t Train time/batch: 62\n",
      "\t Train time/sample: 63488\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 19046400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.6642\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.6635\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0007\n",
      "\t Train time/train: 0.4268\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.4268\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=64/20000]:\n",
      "\t Train time/epoch: 63\n",
      "\t Train time/batch: 63\n",
      "\t Train time/sample: 64512\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 19353600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.6405\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.6398\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0007\n",
      "\t Train time/train: 0.4373\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.4373\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=65/20000]:\n",
      "\t Train time/epoch: 64\n",
      "\t Train time/batch: 64\n",
      "\t Train time/sample: 65536\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 19660800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.6280\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.6281\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0006\n",
      "\t Train time/train: 0.4477\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.4477\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=66/20000]:\n",
      "\t Train time/epoch: 65\n",
      "\t Train time/batch: 65\n",
      "\t Train time/sample: 66560\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 19968000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.6143\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.6143\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0007\n",
      "\t Train time/train: 0.4582\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.4582\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=67/20000]:\n",
      "\t Train time/epoch: 66\n",
      "\t Train time/batch: 66\n",
      "\t Train time/sample: 67584\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 20275200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.5986\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.5991\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0006\n",
      "\t Train time/train: 0.4687\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.4687\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=68/20000]:\n",
      "\t Train time/epoch: 67\n",
      "\t Train time/batch: 67\n",
      "\t Train time/sample: 68608\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 20582400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.5812\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.5811\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0009\n",
      "\t Train time/train: 0.4791\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.4791\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=69/20000]:\n",
      "\t Train time/epoch: 68\n",
      "\t Train time/batch: 68\n",
      "\t Train time/sample: 69632\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 20889600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.5720\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.5710\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0006\n",
      "\t Train time/train: 0.4896\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.4896\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=70/20000]:\n",
      "\t Train time/epoch: 69\n",
      "\t Train time/batch: 69\n",
      "\t Train time/sample: 70656\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 21196800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.5532\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.5533\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0006\n",
      "\t Train time/train: 0.5000\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.5000\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=71/20000]:\n",
      "\t Train time/epoch: 70\n",
      "\t Train time/batch: 70\n",
      "\t Train time/sample: 71680\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 21504000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.5352\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.5353\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0012\n",
      "\t Train time/train: 0.5105\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.5105\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=72/20000]:\n",
      "\t Train time/epoch: 71\n",
      "\t Train time/batch: 71\n",
      "\t Train time/sample: 72704\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 21811200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.5247\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.5246\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0013\n",
      "\t Train time/train: 0.5209\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.5209\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=73/20000]:\n",
      "\t Train time/epoch: 72\n",
      "\t Train time/batch: 72\n",
      "\t Train time/sample: 73728\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 22118400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.5200\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.5201\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0007\n",
      "\t Train time/train: 0.5314\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.5314\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=74/20000]:\n",
      "\t Train time/epoch: 73\n",
      "\t Train time/batch: 73\n",
      "\t Train time/sample: 74752\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 22425600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.5133\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.5130\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0008\n",
      "\t Train time/train: 0.5418\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.5418\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=75/20000]:\n",
      "\t Train time/epoch: 74\n",
      "\t Train time/batch: 74\n",
      "\t Train time/sample: 75776\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 22732800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.4938\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.4937\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0010\n",
      "\t Train time/train: 0.5523\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.5523\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=76/20000]:\n",
      "\t Train time/epoch: 75\n",
      "\t Train time/batch: 75\n",
      "\t Train time/sample: 76800\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 23040000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.4818\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.4823\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0006\n",
      "\t Train time/train: 0.5627\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.5627\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=77/20000]:\n",
      "\t Train time/epoch: 76\n",
      "\t Train time/batch: 76\n",
      "\t Train time/sample: 77824\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 23347200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.4779\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.4789\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0010\n",
      "\t Train time/train: 0.5731\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.5731\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=78/20000]:\n",
      "\t Train time/epoch: 77\n",
      "\t Train time/batch: 77\n",
      "\t Train time/sample: 78848\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 23654400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.4667\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.4663\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0011\n",
      "\t Train time/train: 0.5836\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.5836\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=79/20000]:\n",
      "\t Train time/epoch: 78\n",
      "\t Train time/batch: 78\n",
      "\t Train time/sample: 79872\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 23961600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.4549\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.4549\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0007\n",
      "\t Train time/train: 0.5940\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.5940\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=80/20000]:\n",
      "\t Train time/epoch: 79\n",
      "\t Train time/batch: 79\n",
      "\t Train time/sample: 80896\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 24268800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.4446\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.4446\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0010\n",
      "\t Train time/train: 0.6045\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.6045\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=81/20000]:\n",
      "\t Train time/epoch: 80\n",
      "\t Train time/batch: 80\n",
      "\t Train time/sample: 81920\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 24576000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.4348\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.4355\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0009\n",
      "\t Train time/train: 0.6150\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.6150\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=82/20000]:\n",
      "\t Train time/epoch: 81\n",
      "\t Train time/batch: 81\n",
      "\t Train time/sample: 82944\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 24883200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.4263\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.4259\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0010\n",
      "\t Train time/train: 0.6254\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.6254\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=83/20000]:\n",
      "\t Train time/epoch: 82\n",
      "\t Train time/batch: 82\n",
      "\t Train time/sample: 83968\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 25190400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.4133\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.4132\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0009\n",
      "\t Train time/train: 0.6358\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.6358\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=84/20000]:\n",
      "\t Train time/epoch: 83\n",
      "\t Train time/batch: 83\n",
      "\t Train time/sample: 84992\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 25497600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.4133\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.4129\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0010\n",
      "\t Train time/train: 0.6462\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.6462\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=85/20000]:\n",
      "\t Train time/epoch: 84\n",
      "\t Train time/batch: 84\n",
      "\t Train time/sample: 86016\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 25804800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3998\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3996\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0011\n",
      "\t Train time/train: 0.6567\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.6567\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=86/20000]:\n",
      "\t Train time/epoch: 85\n",
      "\t Train time/batch: 85\n",
      "\t Train time/sample: 87040\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 26112000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3991\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3986\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0010\n",
      "\t Train time/train: 0.6671\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.6671\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=87/20000]:\n",
      "\t Train time/epoch: 86\n",
      "\t Train time/batch: 86\n",
      "\t Train time/sample: 88064\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 26419200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3882\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3884\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0012\n",
      "\t Train time/train: 0.6775\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.6775\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=88/20000]:\n",
      "\t Train time/epoch: 87\n",
      "\t Train time/batch: 87\n",
      "\t Train time/sample: 89088\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 26726400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3828\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3825\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0010\n",
      "\t Train time/train: 0.6879\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.6879\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=89/20000]:\n",
      "\t Train time/epoch: 88\n",
      "\t Train time/batch: 88\n",
      "\t Train time/sample: 90112\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 27033600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3764\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3763\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0017\n",
      "\t Train time/train: 0.6984\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.6984\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=90/20000]:\n",
      "\t Train time/epoch: 89\n",
      "\t Train time/batch: 89\n",
      "\t Train time/sample: 91136\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 27340800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3685\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3688\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0008\n",
      "\t Train time/train: 0.7088\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.7088\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=91/20000]:\n",
      "\t Train time/epoch: 90\n",
      "\t Train time/batch: 90\n",
      "\t Train time/sample: 92160\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 27648000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3619\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3626\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0012\n",
      "\t Train time/train: 0.7193\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.7193\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=92/20000]:\n",
      "\t Train time/epoch: 91\n",
      "\t Train time/batch: 91\n",
      "\t Train time/sample: 93184\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 27955200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3581\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3582\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0013\n",
      "\t Train time/train: 0.7297\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.7297\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=93/20000]:\n",
      "\t Train time/epoch: 92\n",
      "\t Train time/batch: 92\n",
      "\t Train time/sample: 94208\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 28262400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3507\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3506\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0011\n",
      "\t Train time/train: 0.7401\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.7401\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=94/20000]:\n",
      "\t Train time/epoch: 93\n",
      "\t Train time/batch: 93\n",
      "\t Train time/sample: 95232\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 28569600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3479\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3483\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0014\n",
      "\t Train time/train: 0.7506\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.7506\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=95/20000]:\n",
      "\t Train time/epoch: 94\n",
      "\t Train time/batch: 94\n",
      "\t Train time/sample: 96256\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 28876800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3423\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3426\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0008\n",
      "\t Train time/train: 0.7610\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.7610\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=96/20000]:\n",
      "\t Train time/epoch: 95\n",
      "\t Train time/batch: 95\n",
      "\t Train time/sample: 97280\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 29184000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3386\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3377\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0013\n",
      "\t Train time/train: 0.7715\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.7715\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=97/20000]:\n",
      "\t Train time/epoch: 96\n",
      "\t Train time/batch: 96\n",
      "\t Train time/sample: 98304\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 29491200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3374\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3372\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0010\n",
      "\t Train time/train: 0.7820\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.7820\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=98/20000]:\n",
      "\t Train time/epoch: 97\n",
      "\t Train time/batch: 97\n",
      "\t Train time/sample: 99328\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 29798400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3326\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3330\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0015\n",
      "\t Train time/train: 0.7924\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.7924\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=99/20000]:\n",
      "\t Train time/epoch: 98\n",
      "\t Train time/batch: 98\n",
      "\t Train time/sample: 100352\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 30105600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3287\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3296\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0013\n",
      "\t Train time/train: 0.8029\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.8029\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=100/20000]:\n",
      "\t Train time/epoch: 99\n",
      "\t Train time/batch: 99\n",
      "\t Train time/sample: 101376\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 30412800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3332\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3323\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0011\n",
      "\t Train time/train: 0.8133\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.8133\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=101/20000]:\n",
      "\t Train time/epoch: 100\n",
      "\t Train time/batch: 100\n",
      "\t Train time/sample: 102400\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 30720000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3105\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3110\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0009\n",
      "\t Train time/train: 0.8237\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.8237\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=102/20000]:\n",
      "\t Train time/epoch: 101\n",
      "\t Train time/batch: 101\n",
      "\t Train time/sample: 103424\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 31027200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3168\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3170\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0015\n",
      "\t Train time/train: 0.8341\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.8341\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=103/20000]:\n",
      "\t Train time/epoch: 102\n",
      "\t Train time/batch: 102\n",
      "\t Train time/sample: 104448\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 31334400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3115\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3117\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0018\n",
      "\t Train time/train: 0.8446\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.8446\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=104/20000]:\n",
      "\t Train time/epoch: 103\n",
      "\t Train time/batch: 103\n",
      "\t Train time/sample: 105472\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 31641600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3130\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3125\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0010\n",
      "\t Train time/train: 0.8550\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.8550\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=105/20000]:\n",
      "\t Train time/epoch: 104\n",
      "\t Train time/batch: 104\n",
      "\t Train time/sample: 106496\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 31948800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3081\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3085\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0007\n",
      "\t Train time/train: 0.8655\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.8655\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=106/20000]:\n",
      "\t Train time/epoch: 105\n",
      "\t Train time/batch: 105\n",
      "\t Train time/sample: 107520\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 32256000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3019\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3012\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0014\n",
      "\t Train time/train: 0.8759\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.8759\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=107/20000]:\n",
      "\t Train time/epoch: 106\n",
      "\t Train time/batch: 106\n",
      "\t Train time/sample: 108544\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 32563200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2976\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2978\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0013\n",
      "\t Train time/train: 0.8864\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.8864\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=108/20000]:\n",
      "\t Train time/epoch: 107\n",
      "\t Train time/batch: 107\n",
      "\t Train time/sample: 109568\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 32870400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3014\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3010\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0013\n",
      "\t Train time/train: 0.8969\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.8969\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=109/20000]:\n",
      "\t Train time/epoch: 108\n",
      "\t Train time/batch: 108\n",
      "\t Train time/sample: 110592\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 33177600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3007\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3011\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0009\n",
      "\t Train time/train: 0.9073\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.9073\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=110/20000]:\n",
      "\t Train time/epoch: 109\n",
      "\t Train time/batch: 109\n",
      "\t Train time/sample: 111616\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 33484800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.3046\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.3040\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0012\n",
      "\t Train time/train: 0.9177\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.9177\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=111/20000]:\n",
      "\t Train time/epoch: 110\n",
      "\t Train time/batch: 110\n",
      "\t Train time/sample: 112640\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 33792000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2955\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2952\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0012\n",
      "\t Train time/train: 0.9282\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.9282\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=112/20000]:\n",
      "\t Train time/epoch: 111\n",
      "\t Train time/batch: 111\n",
      "\t Train time/sample: 113664\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 34099200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2913\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2905\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0011\n",
      "\t Train time/train: 0.9386\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.9386\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=113/20000]:\n",
      "\t Train time/epoch: 112\n",
      "\t Train time/batch: 112\n",
      "\t Train time/sample: 114688\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 34406400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2874\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2865\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0017\n",
      "\t Train time/train: 0.9491\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.9491\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=114/20000]:\n",
      "\t Train time/epoch: 113\n",
      "\t Train time/batch: 113\n",
      "\t Train time/sample: 115712\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 34713600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2787\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2789\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0010\n",
      "\t Train time/train: 0.9595\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.9595\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=115/20000]:\n",
      "\t Train time/epoch: 114\n",
      "\t Train time/batch: 114\n",
      "\t Train time/sample: 116736\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 35020800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2904\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2908\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0014\n",
      "\t Train time/train: 0.9699\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.9699\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=116/20000]:\n",
      "\t Train time/epoch: 115\n",
      "\t Train time/batch: 115\n",
      "\t Train time/sample: 117760\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 35328000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2862\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2864\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0009\n",
      "\t Train time/train: 0.9804\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.9804\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=117/20000]:\n",
      "\t Train time/epoch: 116\n",
      "\t Train time/batch: 116\n",
      "\t Train time/sample: 118784\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 35635200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2886\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2883\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0014\n",
      "\t Train time/train: 0.9908\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.9908\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=118/20000]:\n",
      "\t Train time/epoch: 117\n",
      "\t Train time/batch: 117\n",
      "\t Train time/sample: 119808\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 35942400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2815\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2822\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0009\n",
      "\t Train time/train: 1.0013\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.0013\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=119/20000]:\n",
      "\t Train time/epoch: 118\n",
      "\t Train time/batch: 118\n",
      "\t Train time/sample: 120832\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 36249600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2704\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2717\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0011\n",
      "\t Train time/train: 1.0117\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.0117\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=120/20000]:\n",
      "\t Train time/epoch: 119\n",
      "\t Train time/batch: 119\n",
      "\t Train time/sample: 121856\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 36556800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2780\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2780\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0014\n",
      "\t Train time/train: 1.0221\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.0221\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=121/20000]:\n",
      "\t Train time/epoch: 120\n",
      "\t Train time/batch: 120\n",
      "\t Train time/sample: 122880\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 36864000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2847\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2845\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0008\n",
      "\t Train time/train: 1.0326\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.0326\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=122/20000]:\n",
      "\t Train time/epoch: 121\n",
      "\t Train time/batch: 121\n",
      "\t Train time/sample: 123904\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 37171200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2753\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2759\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0006\n",
      "\t Train time/train: 1.0431\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.0431\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=123/20000]:\n",
      "\t Train time/epoch: 122\n",
      "\t Train time/batch: 122\n",
      "\t Train time/sample: 124928\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 37478400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2819\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2826\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0012\n",
      "\t Train time/train: 1.0535\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.0535\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=124/20000]:\n",
      "\t Train time/epoch: 123\n",
      "\t Train time/batch: 123\n",
      "\t Train time/sample: 125952\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 37785600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2742\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2741\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0022\n",
      "\t Train time/train: 1.0640\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.0640\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=125/20000]:\n",
      "\t Train time/epoch: 124\n",
      "\t Train time/batch: 124\n",
      "\t Train time/sample: 126976\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 38092800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2731\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2726\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0015\n",
      "\t Train time/train: 1.0744\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.0744\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=126/20000]:\n",
      "\t Train time/epoch: 125\n",
      "\t Train time/batch: 125\n",
      "\t Train time/sample: 128000\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 38400000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2800\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2800\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0014\n",
      "\t Train time/train: 1.0849\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.0849\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=127/20000]:\n",
      "\t Train time/epoch: 126\n",
      "\t Train time/batch: 126\n",
      "\t Train time/sample: 129024\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 38707200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2737\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2727\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0015\n",
      "\t Train time/train: 1.0954\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.0954\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=128/20000]:\n",
      "\t Train time/epoch: 127\n",
      "\t Train time/batch: 127\n",
      "\t Train time/sample: 130048\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 39014400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2784\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2775\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0012\n",
      "\t Train time/train: 1.1058\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.1058\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=129/20000]:\n",
      "\t Train time/epoch: 128\n",
      "\t Train time/batch: 128\n",
      "\t Train time/sample: 131072\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 39321600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2798\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2806\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0016\n",
      "\t Train time/train: 1.1162\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.1162\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=130/20000]:\n",
      "\t Train time/epoch: 129\n",
      "\t Train time/batch: 129\n",
      "\t Train time/sample: 132096\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 39628800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2721\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2734\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0006\n",
      "\t Train time/train: 1.1267\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.1267\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=131/20000]:\n",
      "\t Train time/epoch: 130\n",
      "\t Train time/batch: 130\n",
      "\t Train time/sample: 133120\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 39936000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2754\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2750\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0022\n",
      "\t Train time/train: 1.1371\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.1371\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=132/20000]:\n",
      "\t Train time/epoch: 131\n",
      "\t Train time/batch: 131\n",
      "\t Train time/sample: 134144\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 40243200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2767\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2766\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0010\n",
      "\t Train time/train: 1.1476\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.1476\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=133/20000]:\n",
      "\t Train time/epoch: 132\n",
      "\t Train time/batch: 132\n",
      "\t Train time/sample: 135168\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 40550400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2740\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2730\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0010\n",
      "\t Train time/train: 1.1580\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.1580\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=134/20000]:\n",
      "\t Train time/epoch: 133\n",
      "\t Train time/batch: 133\n",
      "\t Train time/sample: 136192\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 40857600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2789\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2796\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0012\n",
      "\t Train time/train: 1.1685\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.1685\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=135/20000]:\n",
      "\t Train time/epoch: 134\n",
      "\t Train time/batch: 134\n",
      "\t Train time/sample: 137216\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 41164800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2810\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2821\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0007\n",
      "\t Train time/train: 1.1789\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.1789\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=136/20000]:\n",
      "\t Train time/epoch: 135\n",
      "\t Train time/batch: 135\n",
      "\t Train time/sample: 138240\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 41472000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2702\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2695\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0013\n",
      "\t Train time/train: 1.1893\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.1893\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=137/20000]:\n",
      "\t Train time/epoch: 136\n",
      "\t Train time/batch: 136\n",
      "\t Train time/sample: 139264\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 41779200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2684\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2683\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0015\n",
      "\t Train time/train: 1.1998\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.1998\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=138/20000]:\n",
      "\t Train time/epoch: 137\n",
      "\t Train time/batch: 137\n",
      "\t Train time/sample: 140288\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 42086400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2710\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2704\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0009\n",
      "\t Train time/train: 1.2102\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.2102\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=139/20000]:\n",
      "\t Train time/epoch: 138\n",
      "\t Train time/batch: 138\n",
      "\t Train time/sample: 141312\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 42393600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2739\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2742\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0012\n",
      "\t Train time/train: 1.2206\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.2206\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=140/20000]:\n",
      "\t Train time/epoch: 139\n",
      "\t Train time/batch: 139\n",
      "\t Train time/sample: 142336\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 42700800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2650\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2648\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0010\n",
      "\t Train time/train: 1.2310\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.2310\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=141/20000]:\n",
      "\t Train time/epoch: 140\n",
      "\t Train time/batch: 140\n",
      "\t Train time/sample: 143360\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 43008000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2743\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2744\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0013\n",
      "\t Train time/train: 1.2415\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.2415\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=142/20000]:\n",
      "\t Train time/epoch: 141\n",
      "\t Train time/batch: 141\n",
      "\t Train time/sample: 144384\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 43315200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2770\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2775\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0009\n",
      "\t Train time/train: 1.2520\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.2520\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=143/20000]:\n",
      "\t Train time/epoch: 142\n",
      "\t Train time/batch: 142\n",
      "\t Train time/sample: 145408\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 43622400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2710\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2728\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0015\n",
      "\t Train time/train: 1.2624\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.2624\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=144/20000]:\n",
      "\t Train time/epoch: 143\n",
      "\t Train time/batch: 143\n",
      "\t Train time/sample: 146432\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 43929600\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2802\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2792\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0004\n",
      "\t Train time/train: 1.2729\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.2729\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=145/20000]:\n",
      "\t Train time/epoch: 144\n",
      "\t Train time/batch: 144\n",
      "\t Train time/sample: 147456\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 44236800\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2723\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2725\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0009\n",
      "\t Train time/train: 1.2833\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.2833\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=146/20000]:\n",
      "\t Train time/epoch: 145\n",
      "\t Train time/batch: 145\n",
      "\t Train time/sample: 148480\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 44544000\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2768\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2782\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0014\n",
      "\t Train time/train: 1.2938\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.2938\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=147/20000]:\n",
      "\t Train time/epoch: 146\n",
      "\t Train time/batch: 146\n",
      "\t Train time/sample: 149504\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 44851200\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2773\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2775\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0017\n",
      "\t Train time/train: 1.3042\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.3042\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=148/20000]:\n",
      "\t Train time/epoch: 147\n",
      "\t Train time/batch: 147\n",
      "\t Train time/sample: 150528\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train time/token: 45158400\n",
      "\t Train time/token_in_epoch: 0\n",
      "\t Train trainer/device_train_microbatch_size: 16\n",
      "\t Train loss/train/total: 8.2755\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2752\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0014\n",
      "\t Train time/train: 1.3147\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 1.3147\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf as om\n",
    "from omegaconf import DictConfig\n",
    "from typing import cast\n",
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from main import main\n",
    "\n",
    "yaml_path = \"../yamls/pretrain/batchlarge_dna_monarch-mixer-pretrain-786dim-80m-parameters.yaml\"\n",
    "\n",
    "with open(yaml_path) as f:\n",
    "    cfg = om.load(f)\n",
    "cfg = cast(DictConfig, cfg)\n",
    "print(cfg.max_duration)\n",
    "main(cfg, False, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda-m2-mixer]",
   "language": "python",
   "name": "conda-env-anaconda-m2-mixer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
