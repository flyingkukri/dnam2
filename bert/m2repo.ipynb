{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('monarch/dnam2/bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.models import HuggingFaceModel\n",
    "\n",
    "model, tokenizer = HuggingFaceModel.hf_from_composer_checkpoint(\n",
    "    f'local-bert-checkpoints/monarch-mixer-pretrain-786dim-80m-parameters/latest-rank0.pt',\n",
    "    model_instantiation_class='transformers.BertForMaskedLM',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CrossEntropy': CrossEntropy(), 'MulticlassAccuracy': MulticlassAccuracy()}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from composer.metrics import CrossEntropy\n",
    "metrics = [CrossEntropy(), MulticlassAccuracy(num_classes=4, average='micro')]\n",
    "composer_model = HuggingFaceModel(model, tokenizer=tokenizer, metrics=metrics, use_logits=True)\n",
    "\n",
    "composer_model.train_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000ba\n",
      "Training using config: \n",
      "data_local: ./300/\n",
      "data_remote: null\n",
      "max_seq_len: 512\n",
      "tokenizer_name: gagneurlab/SpeciesLM\n",
      "mlm_probability: 0.3\n",
      "run_name: monarch-mixer-pretrain-786dim-80m-parameters\n",
      "model:\n",
      "  name: bert\n",
      "  pretrained_model_name: bert-base-uncased\n",
      "  tokenizer_name: ${tokenizer_name}\n",
      "  model_config:\n",
      "    num_attention_heads: 12\n",
      "    num_hidden_layers: 12\n",
      "    attention_probs_dropout_prob: 0.0\n",
      "    max_position_embeddings: ${max_seq_len}\n",
      "    monarch_mixer_sequence_mixing: true\n",
      "    long_conv_l_max: ${max_seq_len}\n",
      "    long_conv_kernel_learning_rate: 0.001\n",
      "    hyena_lr_pos_emb: 1.0e-05\n",
      "    hyena_w: 10\n",
      "    hyena_wd: 0.1\n",
      "    hyena_emb_dim: 5\n",
      "    hyena_filter_order: 128\n",
      "    bidirectional: true\n",
      "    residual_long_conv: true\n",
      "    use_glu_mlp: true\n",
      "    use_monarch_mlp: true\n",
      "    monarch_mlp_nblocks: 4\n",
      "    use_positional_encodings: true\n",
      "train_loader:\n",
      "  name: dna\n",
      "  dataset:\n",
      "    local: ${data_local}\n",
      "    remote: null\n",
      "    split: train\n",
      "    tokenizer_name: ${tokenizer_name}\n",
      "    max_seq_len: ${max_seq_len}\n",
      "    shuffle: true\n",
      "    mlm_probability: ${mlm_probability}\n",
      "  drop_last: true\n",
      "  num_workers: 8\n",
      "eval_loader:\n",
      "  name: dna\n",
      "  dataset:\n",
      "    local: ${data_local}\n",
      "    remote: null\n",
      "    split: test\n",
      "    tokenizer_name: ${tokenizer_name}\n",
      "    max_seq_len: ${max_seq_len}\n",
      "    shuffle: false\n",
      "    mlm_probability: 0.15\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "scheduler:\n",
      "  name: linear_decay_with_warmup\n",
      "  t_warmup: 0.06dur\n",
      "  alpha_f: 0.02\n",
      "precision: fp32\n",
      "optimizer:\n",
      "  name: decoupled_adamw\n",
      "  lr: 0.0008\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.98\n",
      "  eps: 1.0e-06\n",
      "  weight_decay: 1.0e-05\n",
      "algorithms:\n",
      "  gradient_clipping:\n",
      "    clipping_type: norm\n",
      "    clipping_threshold: 1.0\n",
      "max_duration: 200000ba\n",
      "eval_interval: 2000ba\n",
      "global_train_batch_size: 8\n",
      "seed: 17\n",
      "device_eval_batch_size: 128\n",
      "device_train_microbatch_size: 128\n",
      "progress_bar: false\n",
      "log_to_console: true\n",
      "console_log_interval: 1ba\n",
      "callbacks:\n",
      "  speed_monitor:\n",
      "    window_size: 500\n",
      "  lr_monitor: {}\n",
      "save_interval: 7000ba\n",
      "save_num_checkpoints_to_keep: 10\n",
      "save_folder: ./local-bert-checkpoints/${run_name}\n",
      "\n",
      "WARNING: device_train_microbatch_size > device_train_batch_size, will be reduced from 128 -> 8.\n",
      "Initializing model...\n",
      "Using Monarch Mixer for Sequence Mixing: True\n",
      "-- Bidirectional: True\n",
      "-- Using Long Conv Residual: True\n",
      "-- Hyena w: 10\n",
      "-- Hyena w mod: 1\n",
      "-- Hyena filter order: 128\n",
      "-- Hyena filter dropout: 0.2\n",
      "-- Hyena filter wd: 0.1\n",
      "-- Hyena filter emb dim: 5\n",
      "-- Hyena filter lr: 0.001\n",
      "-- Hyena filter lr pos emb: 1e-05\n",
      "n_params=8.0608e+07\n",
      "Building train loader...\n",
      "Building eval loader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/composer/callbacks/speed_monitor.py:120: UserWarning: gpu_flop count not found for None with precision: fp32; MFU cannot be calculated and reported. gpu_flops_available can be manuallyoverridden by setting gpu_flops_available in SpeedMonitor.\n",
      "  warnings.warn(\n",
      "******************************\n",
      "Config:\n",
      "enabled_algorithms/GradientClipping: true\n",
      "node_name: unknown because NODENAME environment variable not set\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 17\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging config...\n",
      "data_local: ./300/\n",
      "data_remote: null\n",
      "max_seq_len: 512\n",
      "tokenizer_name: gagneurlab/SpeciesLM\n",
      "mlm_probability: 0.3\n",
      "run_name: monarch-mixer-pretrain-786dim-80m-parameters\n",
      "model:\n",
      "  name: bert\n",
      "  pretrained_model_name: bert-base-uncased\n",
      "  tokenizer_name: ${tokenizer_name}\n",
      "  model_config:\n",
      "    num_attention_heads: 12\n",
      "    num_hidden_layers: 12\n",
      "    attention_probs_dropout_prob: 0.0\n",
      "    max_position_embeddings: ${max_seq_len}\n",
      "    monarch_mixer_sequence_mixing: true\n",
      "    long_conv_l_max: ${max_seq_len}\n",
      "    long_conv_kernel_learning_rate: 0.001\n",
      "    hyena_lr_pos_emb: 1.0e-05\n",
      "    hyena_w: 10\n",
      "    hyena_wd: 0.1\n",
      "    hyena_emb_dim: 5\n",
      "    hyena_filter_order: 128\n",
      "    bidirectional: true\n",
      "    residual_long_conv: true\n",
      "    use_glu_mlp: true\n",
      "    use_monarch_mlp: true\n",
      "    monarch_mlp_nblocks: 4\n",
      "    use_positional_encodings: true\n",
      "train_loader:\n",
      "  name: dna\n",
      "  dataset:\n",
      "    local: ${data_local}\n",
      "    remote: null\n",
      "    split: train\n",
      "    tokenizer_name: ${tokenizer_name}\n",
      "    max_seq_len: ${max_seq_len}\n",
      "    shuffle: true\n",
      "    mlm_probability: ${mlm_probability}\n",
      "  drop_last: true\n",
      "  num_workers: 8\n",
      "eval_loader:\n",
      "  name: dna\n",
      "  dataset:\n",
      "    local: ${data_local}\n",
      "    remote: null\n",
      "    split: test\n",
      "    tokenizer_name: ${tokenizer_name}\n",
      "    max_seq_len: ${max_seq_len}\n",
      "    shuffle: false\n",
      "    mlm_probability: 0.15\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "scheduler:\n",
      "  name: linear_decay_with_warmup\n",
      "  t_warmup: 0.06dur\n",
      "  alpha_f: 0.02\n",
      "precision: fp32\n",
      "optimizer:\n",
      "  name: decoupled_adamw\n",
      "  lr: 0.0008\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.98\n",
      "  eps: 1.0e-06\n",
      "  weight_decay: 1.0e-05\n",
      "algorithms:\n",
      "  gradient_clipping:\n",
      "    clipping_type: norm\n",
      "    clipping_threshold: 1.0\n",
      "max_duration: 200000ba\n",
      "eval_interval: 2000ba\n",
      "global_train_batch_size: 8\n",
      "seed: 17\n",
      "device_eval_batch_size: 128\n",
      "device_train_microbatch_size: 8\n",
      "progress_bar: false\n",
      "log_to_console: true\n",
      "console_log_interval: 1ba\n",
      "callbacks:\n",
      "  speed_monitor:\n",
      "    window_size: 500\n",
      "  lr_monitor: {}\n",
      "save_interval: 7000ba\n",
      "save_num_checkpoints_to_keep: 10\n",
      "save_folder: ./local-bert-checkpoints/${run_name}\n",
      "n_gpus: 1\n",
      "device_train_batch_size: 8\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[batch=36625/200000]:\n",
      "\t Train time/batch: 36624\n",
      "\t Train time/sample: 292992\n",
      "\t Train time/batch_in_epoch: 1624\n",
      "\t Train time/sample_in_epoch: 12992\n",
      "\t Train time/token: 87311616\n",
      "\t Train time/token_in_epoch: 3871616\n",
      "\t Train trainer/device_train_microbatch_size: 8\n",
      "\t Train loss/train/total: 8.2408\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2408\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0042\n",
      "\t Train time/train: 2.3213\n",
      "\t Train time/val: 7.1934\n",
      "\t Train time/total: 9.5147\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0007\n",
      "[batch=36626/200000]:\n",
      "\t Train time/batch: 36625\n",
      "\t Train time/sample: 293000\n",
      "\t Train time/batch_in_epoch: 1625\n",
      "\t Train time/sample_in_epoch: 13000\n",
      "\t Train time/token: 87314000\n",
      "\t Train time/token_in_epoch: 3874000\n",
      "\t Train trainer/device_train_microbatch_size: 8\n",
      "\t Train loss/train/total: 8.2682\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2682\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0029\n",
      "\t Train time/train: 2.3214\n",
      "\t Train time/val: 7.1934\n",
      "\t Train time/total: 9.5148\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0007\n",
      "[batch=36627/200000]:\n",
      "\t Train time/batch: 36626\n",
      "\t Train time/sample: 293008\n",
      "\t Train time/batch_in_epoch: 1626\n",
      "\t Train time/sample_in_epoch: 13008\n",
      "\t Train time/token: 87316384\n",
      "\t Train time/token_in_epoch: 3876384\n",
      "\t Train trainer/device_train_microbatch_size: 8\n",
      "\t Train loss/train/total: 8.2206\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2206\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0000\n",
      "\t Train time/train: 2.3214\n",
      "\t Train time/val: 7.1934\n",
      "\t Train time/total: 9.5148\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0007\n",
      "[batch=36628/200000]:\n",
      "\t Train time/batch: 36627\n",
      "\t Train time/sample: 293016\n",
      "\t Train time/batch_in_epoch: 1627\n",
      "\t Train time/sample_in_epoch: 13016\n",
      "\t Train time/token: 87318768\n",
      "\t Train time/token_in_epoch: 3878768\n",
      "\t Train trainer/device_train_microbatch_size: 8\n",
      "\t Train loss/train/total: 8.2658\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2658\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0027\n",
      "\t Train time/train: 2.3215\n",
      "\t Train time/val: 7.1934\n",
      "\t Train time/total: 9.5149\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0007\n",
      "[batch=36629/200000]:\n",
      "\t Train time/batch: 36628\n",
      "\t Train time/sample: 293024\n",
      "\t Train time/batch_in_epoch: 1628\n",
      "\t Train time/sample_in_epoch: 13024\n",
      "\t Train time/token: 87321152\n",
      "\t Train time/token_in_epoch: 3881152\n",
      "\t Train trainer/device_train_microbatch_size: 8\n",
      "\t Train loss/train/total: 8.2583\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2582\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0014\n",
      "\t Train time/train: 2.3216\n",
      "\t Train time/val: 7.1934\n",
      "\t Train time/total: 9.5149\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0007\n",
      "[batch=36630/200000]:\n",
      "\t Train time/batch: 36629\n",
      "\t Train time/sample: 293032\n",
      "\t Train time/batch_in_epoch: 1629\n",
      "\t Train time/sample_in_epoch: 13032\n",
      "\t Train time/token: 87323536\n",
      "\t Train time/token_in_epoch: 3883536\n",
      "\t Train trainer/device_train_microbatch_size: 8\n",
      "\t Train loss/train/total: 8.2045\n",
      "\t Train metrics/train/LanguageCrossEntropy: 8.2045\n",
      "\t Train metrics/train/MaskedAccuracy: 0.0000\n",
      "\t Train time/train: 2.3216\n",
      "\t Train time/val: 7.1934\n",
      "\t Train time/total: 9.5150\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0007\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m cfg \u001b[38;5;241m=\u001b[39m cast(DictConfig, cfg)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mmax_duration)\n\u001b[0;32m---> 12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/ceph/hdd/project/node_09/semi_supervised_multispecies/monarch/dnam2/bert/main.py:271\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg, return_trainer, do_train)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_train:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 271\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_trainer:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/composer/trainer/trainer.py:1766\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_dataloader, train_dataloader_label, train_subset_num_batches, duration, reset_time, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_subset_num_batches, eval_interval, device_train_microbatch_size, precision)\u001b[0m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m ClosureGradScaler() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_closures() \u001b[38;5;28;01melse\u001b[39;00m GradScaler()\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst_batch_complete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1766\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/composer/trainer/trainer.py:1940\u001b[0m, in \u001b[0;36mTrainer._train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1937\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_metrics({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime/token\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mtimestamp\u001b[38;5;241m.\u001b[39mtoken\u001b[38;5;241m.\u001b[39mvalue})\n\u001b[1;32m   1938\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_metrics({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime/token_in_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mtimestamp\u001b[38;5;241m.\u001b[39mtoken_in_epoch\u001b[38;5;241m.\u001b[39mvalue})\n\u001b[0;32m-> 1940\u001b[0m total_loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_grad_scaling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_grad_scaling:\n\u001b[1;32m   1943\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/composer/trainer/trainer.py:2115\u001b[0m, in \u001b[0;36mTrainer._train_batch\u001b[0;34m(self, use_grad_scaling)\u001b[0m\n\u001b[1;32m   2111\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mstep(optimizer,\n\u001b[1;32m   2112\u001b[0m                                    closure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m loss_dict\u001b[38;5;241m=\u001b[39mtotal_loss_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m   2113\u001b[0m                                    _train_microbatches(microbatches, loss_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m   2114\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2115\u001b[0m             \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_microbatches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmicrobatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_loss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_microbatches(microbatches, total_loss_dict)\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/composer/optim/decoupled_weight_decay.py:288\u001b[0m, in \u001b[0;36mDecoupledAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 288\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    291\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/composer/trainer/trainer.py:2115\u001b[0m, in \u001b[0;36mTrainer._train_batch.<locals>.<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m   2111\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mstep(optimizer,\n\u001b[1;32m   2112\u001b[0m                                    closure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m loss_dict\u001b[38;5;241m=\u001b[39mtotal_loss_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m   2113\u001b[0m                                    _train_microbatches(microbatches, loss_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m   2114\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2115\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_microbatches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmicrobatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_loss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_microbatches(microbatches, total_loss_dict)\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/composer/trainer/trainer.py:2213\u001b[0m, in \u001b[0;36mTrainer._train_microbatches\u001b[0;34m(self, microbatches, total_loss_dict, ddp_sync)\u001b[0m\n\u001b[1;32m   2211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m microbatch_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(microbatches):\n\u001b[1;32m   2212\u001b[0m     is_final_microbatch \u001b[38;5;241m=\u001b[39m microbatch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(microbatches)\n\u001b[0;32m-> 2213\u001b[0m     microbatch_loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_microbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_grad_scaling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_final_microbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2215\u001b[0m     \u001b[38;5;66;03m# Aggregate each loss in microbatch_loss_dict into total_loss_dict\u001b[39;00m\n\u001b[1;32m   2216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, microbatch_loss \u001b[38;5;129;01min\u001b[39;00m microbatch_loss_dict\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/composer/trainer/trainer.py:2340\u001b[0m, in \u001b[0;36mTrainer._train_microbatch\u001b[0;34m(self, use_grad_scaling, current_batch_size, is_final_microbatch)\u001b[0m\n\u001b[1;32m   2337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2338\u001b[0m     \u001b[38;5;66;03m# Scale loss based on the number of samples in the microbatch to maintain gradient numerics\u001b[39;00m\n\u001b[1;32m   2339\u001b[0m     microbatch_loss\u001b[38;5;241m.\u001b[39mmul_(microbatch_num_samples \u001b[38;5;241m/\u001b[39m current_batch_size)\n\u001b[0;32m-> 2340\u001b[0m     \u001b[43mmicrobatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backwards_create_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2342\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mrun_event(Event\u001b[38;5;241m.\u001b[39mAFTER_BACKWARD)\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;66;03m# Use microbatch outputs to update training metrics\u001b[39;00m\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf as om\n",
    "from omegaconf import DictConfig\n",
    "from typing import cast\n",
    "from main import main \n",
    "\n",
    "yaml_path = \"yamls/pretrain/dna_monarch-mixer-pretrain-786dim-80m-parameters.yaml\"\n",
    "\n",
    "with open(yaml_path) as f:\n",
    "    cfg = om.load(f)\n",
    "cfg = cast(DictConfig, cfg)\n",
    "print(cfg.max_duration)\n",
    "trainer = main(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda-m2-mixer]",
   "language": "python",
   "name": "conda-env-anaconda-m2-mixer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
