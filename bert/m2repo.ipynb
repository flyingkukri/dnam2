{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lukas/git/dnam2/bert'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Local path local-bert-checkpoints/monarch-mixer-pretrain-786dim-80m-parameters/latest-rank0.pt does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/m2_bert/lib/python3.10/site-packages/composer/utils/file_helpers.py:504\u001b[0m, in \u001b[0;36mget_file\u001b[0;34m(path, destination, object_store, overwrite, progress_bar)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;66;03m# Follow the symlink\u001b[39;00m\n\u001b[0;32m--> 504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobject_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ee:\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# Raise the original not found error first, which contains the path to the user-specified file\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/m2_bert/lib/python3.10/site-packages/composer/utils/file_helpers.py:471\u001b[0m, in \u001b[0;36mget_file\u001b[0;34m(path, destination, object_store, overwrite, progress_bar)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# Retrieve the symlink\u001b[39;00m\n\u001b[0;32m--> 471\u001b[0m \u001b[43m_get_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdestination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msymlink_file_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# Read object name in the symlink\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/m2_bert/lib/python3.10/site-packages/composer/utils/file_helpers.py:580\u001b[0m, in \u001b[0;36m_get_file\u001b[0;34m(path, destination, object_store, overwrite, progress_bar)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[0;32m--> 580\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocal path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    581\u001b[0m os\u001b[38;5;241m.\u001b[39msymlink(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(path), destination)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Local path local-bert-checkpoints/monarch-mixer-pretrain-786dim-80m-parameters/latest-rank0.pt.symlink does not exist",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomposer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceModel\n\u001b[0;32m----> 3\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_from_composer_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocal-bert-checkpoints/monarch-mixer-pretrain-786dim-80m-parameters/latest-rank0.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_instantiation_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransformers.BertForMaskedLM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/m2_bert/lib/python3.10/site-packages/composer/models/huggingface.py:228\u001b[0m, in \u001b[0;36mHuggingFaceModel.hf_from_composer_checkpoint\u001b[0;34m(checkpoint_path, model_instantiation_class, model_config_kwargs, local_checkpoint_save_location)\u001b[0m\n\u001b[1;32m    225\u001b[0m     model_config_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# download the checkpoint file\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m \u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlocal_checkpoint_save_location\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# load the state dict in\u001b[39;00m\n\u001b[1;32m    231\u001b[0m loaded_state_dict \u001b[38;5;241m=\u001b[39m safe_torch_load(local_checkpoint_save_location)\n",
      "File \u001b[0;32m~/.conda/envs/m2_bert/lib/python3.10/site-packages/composer/utils/file_helpers.py:513\u001b[0m, in \u001b[0;36mget_file\u001b[0;34m(path, destination, object_store, overwrite, progress_bar)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_file(\n\u001b[1;32m    505\u001b[0m         path\u001b[38;5;241m=\u001b[39mnew_path,\n\u001b[1;32m    506\u001b[0m         destination\u001b[38;5;241m=\u001b[39mdestination,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m    510\u001b[0m     )\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ee:\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# Raise the original not found error first, which contains the path to the user-specified file\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mee\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/m2_bert/lib/python3.10/site-packages/composer/utils/file_helpers.py:493\u001b[0m, in \u001b[0;36mget_file\u001b[0;34m(path, destination, object_store, overwrite, progress_bar)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_file(\n\u001b[1;32m    485\u001b[0m         path\u001b[38;5;241m=\u001b[39mreal_path,\n\u001b[1;32m    486\u001b[0m         destination\u001b[38;5;241m=\u001b[39mdestination,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m    490\u001b[0m     )\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 493\u001b[0m     \u001b[43m_get_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobject_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    501\u001b[0m     new_path \u001b[38;5;241m=\u001b[39m path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.symlink\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/m2_bert/lib/python3.10/site-packages/composer/utils/file_helpers.py:580\u001b[0m, in \u001b[0;36m_get_file\u001b[0;34m(path, destination, object_store, overwrite, progress_bar)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;66;03m# It's a local filepath\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[0;32m--> 580\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocal path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    581\u001b[0m os\u001b[38;5;241m.\u001b[39msymlink(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(path), destination)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Local path local-bert-checkpoints/monarch-mixer-pretrain-786dim-80m-parameters/latest-rank0.pt does not exist"
     ]
    }
   ],
   "source": [
    "\n",
    "from composer.models import HuggingFaceModel\n",
    "\n",
    "model, tokenizer = HuggingFaceModel.hf_from_composer_checkpoint(\n",
    "    f'local-bert-checkpoints/monarch-mixer-pretrain-786dim-80m-parameters/latest-rank0.pt',\n",
    "    model_instantiation_class='transformers.BertForMaskedLM',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CrossEntropy': CrossEntropy(), 'MulticlassAccuracy': MulticlassAccuracy()}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from composer.metrics import CrossEntropy\n",
    "metrics = [CrossEntropy(), MulticlassAccuracy(num_classes=4, average='micro')]\n",
    "composer_model = HuggingFaceModel(model, tokenizer=tokenizer, metrics=metrics, use_logits=True)\n",
    "\n",
    "composer_model.train_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not import FlashFFTConv!\n",
      "200000ba\n",
      "Training using config: \n",
      "data_local: ./300/\n",
      "data_remote: null\n",
      "max_seq_len: 512\n",
      "tokenizer_name: gagneurlab/SpeciesLM\n",
      "mlm_probability: 0.02\n",
      "run_name: monarch-mixer-pretrain-786dim-80m-parameters\n",
      "model:\n",
      "  name: bert\n",
      "  pretrained_model_name: bert-base-uncased\n",
      "  tokenizer_name: ${tokenizer_name}\n",
      "  model_config:\n",
      "    num_attention_heads: 12\n",
      "    num_hidden_layers: 12\n",
      "    attention_probs_dropout_prob: 0.0\n",
      "    max_position_embeddings: ${max_seq_len}\n",
      "    monarch_mixer_sequence_mixing: true\n",
      "    long_conv_l_max: ${max_seq_len}\n",
      "    long_conv_kernel_learning_rate: 0.001\n",
      "    hyena_lr_pos_emb: 1.0e-05\n",
      "    hyena_w: 10\n",
      "    hyena_wd: 0.1\n",
      "    hyena_emb_dim: 5\n",
      "    hyena_filter_order: 128\n",
      "    bidirectional: true\n",
      "    residual_long_conv: true\n",
      "    use_glu_mlp: true\n",
      "    use_monarch_mlp: true\n",
      "    monarch_mlp_nblocks: 4\n",
      "    use_positional_encodings: true\n",
      "train_loader:\n",
      "  name: dna\n",
      "  dataset:\n",
      "    local: ${data_local}\n",
      "    remote: null\n",
      "    split: train\n",
      "    tokenizer_name: ${tokenizer_name}\n",
      "    max_seq_len: ${max_seq_len}\n",
      "    shuffle: true\n",
      "    mlm_probability: ${mlm_probability}\n",
      "  drop_last: true\n",
      "  num_workers: 8\n",
      "eval_loader:\n",
      "  name: dna\n",
      "  dataset:\n",
      "    local: ${data_local}\n",
      "    remote: null\n",
      "    split: test\n",
      "    tokenizer_name: ${tokenizer_name}\n",
      "    max_seq_len: ${max_seq_len}\n",
      "    shuffle: false\n",
      "    mlm_probability: 0.15\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "scheduler:\n",
      "  name: linear_decay_with_warmup\n",
      "  t_warmup: 0.0005dur\n",
      "  alpha_f: 0.02\n",
      "precision: fp32\n",
      "optimizer:\n",
      "  name: decoupled_adamw\n",
      "  lr: 0.0004\n",
      "  betas:\n",
      "  - 0.95\n",
      "  - 0.99\n",
      "  eps: 1.0e-06\n",
      "  weight_decay: 1.0e-05\n",
      "algorithms:\n",
      "  gradient_clipping:\n",
      "    clipping_type: norm\n",
      "    clipping_threshold: 1.0\n",
      "max_duration: 200000ba\n",
      "eval_interval: 2000ba\n",
      "global_train_batch_size: 1028\n",
      "seed: 17\n",
      "device_eval_batch_size: 128\n",
      "device_train_microbatch_size: 16\n",
      "progress_bar: false\n",
      "log_to_console: true\n",
      "console_log_interval: 1ba\n",
      "callbacks:\n",
      "  speed_monitor:\n",
      "    window_size: 500\n",
      "  lr_monitor: {}\n",
      "save_interval: 7000ba\n",
      "save_num_checkpoints_to_keep: 10\n",
      "save_folder: ./local-bert-checkpoints/${run_name}\n",
      "\n",
      "Initializing model...\n",
      "Using Monarch Mixer for Sequence Mixing: True\n",
      "-- Bidirectional: True\n",
      "-- Using Long Conv Residual: True\n",
      "-- Hyena w: 10\n",
      "-- Hyena w mod: 1\n",
      "-- Hyena filter order: 128\n",
      "-- Hyena filter dropout: 0.2\n",
      "-- Hyena filter wd: 0.1\n",
      "-- Hyena filter emb dim: 5\n",
      "-- Hyena filter lr: 0.001\n",
      "-- Hyena filter lr pos emb: 1e-05\n",
      "n_params=8.0608e+07\n",
      "Building train loader...\n",
      "==Using the correct data collator\n",
      "Building eval loader...\n",
      "==Using the correct data collator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Config:\n",
      "enabled_algorithms/GradientClipping: true\n",
      "node_name: unknown because NODENAME environment variable not set\n",
      "num_cpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 17\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging config...\n",
      "data_local: ./300/\n",
      "data_remote: null\n",
      "max_seq_len: 512\n",
      "tokenizer_name: gagneurlab/SpeciesLM\n",
      "mlm_probability: 0.02\n",
      "run_name: monarch-mixer-pretrain-786dim-80m-parameters\n",
      "model:\n",
      "  name: bert\n",
      "  pretrained_model_name: bert-base-uncased\n",
      "  tokenizer_name: ${tokenizer_name}\n",
      "  model_config:\n",
      "    num_attention_heads: 12\n",
      "    num_hidden_layers: 12\n",
      "    attention_probs_dropout_prob: 0.0\n",
      "    max_position_embeddings: ${max_seq_len}\n",
      "    monarch_mixer_sequence_mixing: true\n",
      "    long_conv_l_max: ${max_seq_len}\n",
      "    long_conv_kernel_learning_rate: 0.001\n",
      "    hyena_lr_pos_emb: 1.0e-05\n",
      "    hyena_w: 10\n",
      "    hyena_wd: 0.1\n",
      "    hyena_emb_dim: 5\n",
      "    hyena_filter_order: 128\n",
      "    bidirectional: true\n",
      "    residual_long_conv: true\n",
      "    use_glu_mlp: true\n",
      "    use_monarch_mlp: true\n",
      "    monarch_mlp_nblocks: 4\n",
      "    use_positional_encodings: true\n",
      "train_loader:\n",
      "  name: dna\n",
      "  dataset:\n",
      "    local: ${data_local}\n",
      "    remote: null\n",
      "    split: train\n",
      "    tokenizer_name: ${tokenizer_name}\n",
      "    max_seq_len: ${max_seq_len}\n",
      "    shuffle: true\n",
      "    mlm_probability: ${mlm_probability}\n",
      "  drop_last: true\n",
      "  num_workers: 8\n",
      "eval_loader:\n",
      "  name: dna\n",
      "  dataset:\n",
      "    local: ${data_local}\n",
      "    remote: null\n",
      "    split: test\n",
      "    tokenizer_name: ${tokenizer_name}\n",
      "    max_seq_len: ${max_seq_len}\n",
      "    shuffle: false\n",
      "    mlm_probability: 0.15\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "scheduler:\n",
      "  name: linear_decay_with_warmup\n",
      "  t_warmup: 0.0005dur\n",
      "  alpha_f: 0.02\n",
      "precision: fp32\n",
      "optimizer:\n",
      "  name: decoupled_adamw\n",
      "  lr: 0.0004\n",
      "  betas:\n",
      "  - 0.95\n",
      "  - 0.99\n",
      "  eps: 1.0e-06\n",
      "  weight_decay: 1.0e-05\n",
      "algorithms:\n",
      "  gradient_clipping:\n",
      "    clipping_type: norm\n",
      "    clipping_threshold: 1.0\n",
      "max_duration: 200000ba\n",
      "eval_interval: 2000ba\n",
      "global_train_batch_size: 1028\n",
      "seed: 17\n",
      "device_eval_batch_size: 128\n",
      "device_train_microbatch_size: 16\n",
      "progress_bar: false\n",
      "log_to_console: true\n",
      "console_log_interval: 1ba\n",
      "callbacks:\n",
      "  speed_monitor:\n",
      "    window_size: 500\n",
      "  lr_monitor: {}\n",
      "save_interval: 7000ba\n",
      "save_num_checkpoints_to_keep: 10\n",
      "save_folder: ./local-bert-checkpoints/${run_name}\n",
      "n_gpus: 1\n",
      "device_train_batch_size: 1028\n",
      "\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf as om\n",
    "from omegaconf import DictConfig\n",
    "from typing import cast\n",
    "from main import main \n",
    "\n",
    "yaml_path = \"yamls/pretrain/dna_monarch-mixer-pretrain-786dim-80m-parameters.yaml\"\n",
    "\n",
    "with open(yaml_path) as f:\n",
    "    cfg = om.load(f)\n",
    "cfg = cast(DictConfig, cfg)\n",
    "print(cfg.max_duration)\n",
    "trainer = main(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
