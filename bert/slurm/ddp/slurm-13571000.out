You are using a model of type m2_bert to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
You are using a model of type m2_bert to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
You are using a model of type m2_bert to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
You are using a model of type m2_bert to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
You are using a model of type m2_bert to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
You are using a model of type m2_bert to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
wandb: Currently logged in as: luluhu (luluh). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: luluhu (luluh). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: luluhu (luluh). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: luluhu (luluh). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: luluhu (luluh). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: luluhu (luluh). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /data/ceph/hdd/project/node_09/semi_supervised_multispecies/monarch/dnam2/bert/slurm/ddp/wandb/run-20240306_150127-b8a2grdv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fullset_bs1024_6gpu_monarchhf
wandb: ‚≠êÔ∏è View project at https://wandb.ai/luluh/singlesamplednam2
wandb: üöÄ View run at https://wandb.ai/luluh/singlesamplednam2/runs/b8a2grdv
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /data/ceph/hdd/project/node_09/semi_supervised_multispecies/monarch/dnam2/bert/slurm/ddp/wandb/run-20240306_150127-cesrv615
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fullset_bs1024_6gpu_monarchhf
wandb: ‚≠êÔ∏è View project at https://wandb.ai/luluh/singlesamplednam2
wandb: üöÄ View run at https://wandb.ai/luluh/singlesamplednam2/runs/cesrv615
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /data/ceph/hdd/project/node_09/semi_supervised_multispecies/monarch/dnam2/bert/slurm/ddp/wandb/run-20240306_150127-ouig6jr6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fullset_bs1024_6gpu_monarchhf
wandb: ‚≠êÔ∏è View project at https://wandb.ai/luluh/singlesamplednam2
wandb: üöÄ View run at https://wandb.ai/luluh/singlesamplednam2/runs/ouig6jr6
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /data/ceph/hdd/project/node_09/semi_supervised_multispecies/monarch/dnam2/bert/slurm/ddp/wandb/run-20240306_150127-kiomzbmp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fullset_bs1024_6gpu_monarchhf
wandb: ‚≠êÔ∏è View project at https://wandb.ai/luluh/singlesamplednam2
wandb: üöÄ View run at https://wandb.ai/luluh/singlesamplednam2/runs/kiomzbmp
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /data/ceph/hdd/project/node_09/semi_supervised_multispecies/monarch/dnam2/bert/slurm/ddp/wandb/run-20240306_150127-376w2632
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fullset_bs1024_6gpu_monarchhf
wandb: ‚≠êÔ∏è View project at https://wandb.ai/luluh/singlesamplednam2
wandb: üöÄ View run at https://wandb.ai/luluh/singlesamplednam2/runs/376w2632
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /data/ceph/hdd/project/node_09/semi_supervised_multispecies/monarch/dnam2/bert/slurm/ddp/wandb/run-20240306_150127-e7lga4h6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fullset_bs1024_6gpu_monarchhf
wandb: ‚≠êÔ∏è View project at https://wandb.ai/luluh/singlesamplednam2
wandb: üöÄ View run at https://wandb.ai/luluh/singlesamplednam2/runs/e7lga4h6
  0%|          | 0/100000 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 0/100000 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 0/100000 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 0/100000 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 0/100000 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 0/100000 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Traceback (most recent call last):
  File "/data/ceph/hdd/project/node_09/semi_supervised_multispecies/monarch/dnam2/bert/slurm/ddp/train.py", line 81, in <module>
    trainer.train()
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 1927, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 2699, in training_step
    loss = self.compute_loss(model, inputs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 2731, in compute_loss
    outputs = model(**inputs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1358, in forward
    outputs = self.bert(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1013, in forward
    embedding_output = self.embeddings(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 233, in forward
    embeddings = inputs_embeds + token_type_embeddings
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
Traceback (most recent call last):
  File "/data/ceph/hdd/project/node_09/semi_supervised_multispecies/monarch/dnam2/bert/slurm/ddp/train.py", line 81, in <module>
    trainer.train()
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 1927, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 2699, in training_step
    loss = self.compute_loss(model, inputs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 2731, in compute_loss
    outputs = model(**inputs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1358, in forward
    outputs = self.bert(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1013, in forward
    embedding_output = self.embeddings(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 235, in forward
    position_embeddings = self.position_embeddings(position_ids)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/sparse.py", line 160, in forward
    return F.embedding(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
Traceback (most recent call last):
  File "/data/ceph/hdd/project/node_09/semi_supervised_multispecies/monarch/dnam2/bert/slurm/ddp/train.py", line 81, in <module>
    trainer.train()
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 1927, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 2699, in training_step
    loss = self.compute_loss(model, inputs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 2731, in compute_loss
    outputs = model(**inputs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1358, in forward
    outputs = self.bert(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1013, in forward
    embedding_output = self.embeddings(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 238, in forward
    embeddings = self.dropout(embeddings)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: philox_cuda_state for an unexpected CUDA generator used during capture. In regions captured by CUDA graphs, you may only use the default CUDA RNG generator on the device that's current when capture begins. If you need a non-default (user-supplied) generator, or a generator on another device, please file an issue.
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
Traceback (most recent call last):
  File "/data/ceph/hdd/project/node_09/semi_supervised_multispecies/monarch/dnam2/bert/slurm/ddp/train.py", line 81, in <module>
    trainer.train()
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 1927, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 2699, in training_step
    loss = self.compute_loss(model, inputs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 2731, in compute_loss
    outputs = model(**inputs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1358, in forward
    outputs = self.bert(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1013, in forward
    embedding_output = self.embeddings(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 238, in forward
    embeddings = self.dropout(embeddings)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Traceback (most recent call last):
  File "/data/ceph/hdd/project/node_09/semi_supervised_multispecies/monarch/dnam2/bert/slurm/ddp/train.py", line 81, in <module>
    trainer.train()
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 1927, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 2699, in training_step
    loss = self.compute_loss(model, inputs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 2731, in compute_loss
    outputs = model(**inputs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1358, in forward
    outputs = self.bert(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1020, in forward
    encoder_outputs = self.encoder(
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 610, in forward
    layer_outputs = layer_module(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 495, in forward
    self_attention_outputs = self.attention(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 425, in forward
    self_outputs = self.self(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 284, in forward
    mixed_query_layer = self.query(hidden_states)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
Traceback (most recent call last):
  File "/data/ceph/hdd/project/node_09/semi_supervised_multispecies/monarch/dnam2/bert/slurm/ddp/train.py", line 81, in <module>
    trainer.train()
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 1927, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 2699, in training_step
    loss = self.compute_loss(model, inputs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/trainer.py", line 2731, in compute_loss
    outputs = model(**inputs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1358, in forward
    outputs = self.bert(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1020, in forward
    encoder_outputs = self.encoder(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 610, in forward
    layer_outputs = layer_module(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 495, in forward
    self_attention_outputs = self.attention(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 425, in forward
    self_outputs = self.self(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 284, in forward
    mixed_query_layer = self.query(hidden_states)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.015 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: \ 0.015 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: | 0.015 MB of 0.029 MB uploaded (0.000 MB deduped)wandb: üöÄ View run fullset_bs1024_6gpu_monarchhf at: https://wandb.ai/luluh/singlesamplednam2/runs/ouig6jr6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240306_150127-ouig6jr6/logs
wandb: - 0.015 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: \ 0.015 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: | 0.015 MB of 0.039 MB uploaded (0.000 MB deduped)wandb: üöÄ View run fullset_bs1024_6gpu_monarchhf at: https://wandb.ai/luluh/singlesamplednam2/runs/cesrv615
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240306_150127-cesrv615/logs
wandb: - 0.015 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: \ 0.015 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: | 0.015 MB of 0.038 MB uploaded (0.000 MB deduped)wandb: üöÄ View run fullset_bs1024_6gpu_monarchhf at: https://wandb.ai/luluh/singlesamplednam2/runs/376w2632
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240306_150127-376w2632/logs
wandb: - 0.015 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: \ 0.015 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: | 0.015 MB of 0.038 MB uploaded (0.000 MB deduped)wandb: üöÄ View run fullset_bs1024_6gpu_monarchhf at: https://wandb.ai/luluh/singlesamplednam2/runs/e7lga4h6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240306_150127-e7lga4h6/logs
wandb: - 0.015 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: \ 0.015 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: | 0.015 MB of 0.039 MB uploaded (0.000 MB deduped)wandb: / 0.039 MB of 0.039 MB uploaded (0.000 MB deduped)wandb: üöÄ View run fullset_bs1024_6gpu_monarchhf at: https://wandb.ai/luluh/singlesamplednam2/runs/b8a2grdv
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240306_150127-b8a2grdv/logs
wandb: - 0.015 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: \ 0.015 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: | 0.015 MB of 0.037 MB uploaded (0.000 MB deduped)wandb: / 0.037 MB of 0.037 MB uploaded (0.000 MB deduped)wandb: üöÄ View run fullset_bs1024_6gpu_monarchhf at: https://wandb.ai/luluh/singlesamplednam2/runs/kiomzbmp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240306_150127-kiomzbmp/logs
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3112176) of binary: /opt/modules/i12g/anaconda/envs/m2-mixer/bin/python3.9
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3112172) of binary: /opt/modules/i12g/anaconda/envs/m2-mixer/bin/python3.9
Traceback (most recent call last):
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-06_15:01:47
  host      : ouga14.cmm.in.tum.de
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3112176)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-06_15:01:47
  host      : ouga14.cmm.in.tum.de
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3112172)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3112184) of binary: /opt/modules/i12g/anaconda/envs/m2-mixer/bin/python3.9
Traceback (most recent call last):
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/bin/torchrun", line 8, in <module>
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3112180) of binary: /opt/modules/i12g/anaconda/envs/m2-mixer/bin/python3.9
    sys.exit(main())
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-06_15:01:47
  host      : ouga14.cmm.in.tum.de
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3112184)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-06_15:01:47
  host      : ouga14.cmm.in.tum.de
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3112180)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: ouga14: tasks 0,4: Exited with exit code 1
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3112182) of binary: /opt/modules/i12g/anaconda/envs/m2-mixer/bin/python3.9
Traceback (most recent call last):
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-06_15:01:47
  host      : ouga14.cmm.in.tum.de
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3112182)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: ouga14: tasks 3,5: Exited with exit code 1
srun: error: ouga14: task 2: Exited with exit code 1
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3112178) of binary: /opt/modules/i12g/anaconda/envs/m2-mixer/bin/python3.9
Traceback (most recent call last):
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/modules/i12g/anaconda/envs/m2-mixer/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-06_15:01:52
  host      : ouga14.cmm.in.tum.de
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3112178)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: ouga14: task 1: Exited with exit code 1
